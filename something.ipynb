{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# libary to read audio in torch\n",
    "import torchaudio\n",
    "from torch.nn import CTCLoss\n",
    "from jiwer import wer\n",
    "import soundfile as sf\n",
    "\n",
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "ctcloss = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c81c9fc806f4af28d416107d8900e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa2b0d1605b46e89bba8d75ca7128f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6597d5818251467690f100ce68646e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e2908e35f749cfb9322d2554f5a55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6e07f396134800ac81d5a4c9a96b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fb9bdef04a41ba96b70812f0c9e506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# example of tokenizing an audio file\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select an audio file and process it\n",
    "#question: is it possible to process the audio after adding noise to it and backpropagate the loss?\n",
    "audio = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values\n",
    "sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "#sentence we want our model to predict\n",
    "target = \"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target]\n",
    "# convert to tensor logits using the tokenizer\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#load everything to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(logits, targets):\n",
    "    #this function only tested for batch_size = 1\n",
    "    input_lengths = torch.tensor([logits.shape[0]])\n",
    "    target_lengths = torch.tensor([targets.shape[0]])\n",
    "    return ctcloss(logits, targets, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(audio, noise, target_logits, model, reg_weight, ctc_weight, eps):\n",
    "    \"\"\" \n",
    "    Computes the loss of the audio with the current noise added, with a factor to control the size of the noise (via regularization) to allow for backpropagation of the input audio signal.\n",
    "    audio: original audio after processing (for now)\n",
    "    noise: noise to be added to the audio\n",
    "    target_logits: target logits for the sentence we want to generate an attack for\n",
    "    model: model to be attacked\n",
    "    reg_weight: weight for the noise regularization term\n",
    "    ctc_weight: weight for the ctc loss\n",
    "    eps: maximum perturbation allowed\n",
    "    \"\"\"\n",
    "    audio_perturbed = audio + noise\n",
    "    #compute dB_x\n",
    "    dB_x = (20 * torch.log10(audio-audio.min())).max()\n",
    "    #compute dB_delta\n",
    "    dB_delta = (20 * torch.log10(noise-noise.min())).max()\n",
    "    #compute dB_x_delta\n",
    "    dB_x_delta = dB_delta - dB_x\n",
    "    #compute logits\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = softmax(logits[0])\n",
    "    logits = logits.unsqueeze(1)\n",
    "    #compute ctc loss\n",
    "    ctc_loss_value = ctc_loss(logits, target_logits)\n",
    "    #compute noise regularization\n",
    "    noise_reg = torch.norm(noise, p=2)\n",
    "    #compute total loss\n",
    "    loss = reg_weight * noise_reg + ctc_weight * ctc_loss_value\n",
    "    if dB_x_delta < eps:\n",
    "        return loss, ctc_loss_value.item(), noise_reg.item(), dB_x_delta.item()\n",
    "    else:\n",
    "        print(loss.item(), ctc_loss_value.item(), noise_reg.item(), dB_x_delta.item())\n",
    "        return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139.14, 27.83, 0.0, -inf]]\n",
      "[[152.54, 24.39, 30.61, -35.37], [116.22, 13.8, 47.2, -29.34], [116.03, 11.28, 59.61, -25.81], [116.64, 9.3, 70.12, -23.31], [124.75, 9.11, 79.22, -21.49], [131.43, 8.93, 86.8, -20.01], [134.92, 8.29, 93.47, -18.81], [141.32, 8.42, 99.23, -17.75], [144.99, 8.15, 104.22, -16.89], [147.14, 7.72, 108.54, -16.24]]\n",
      "[[150.55, 7.68, 112.17, -15.56], [153.27, 7.58, 115.39, -15.04], [155.35, 7.43, 118.2, -14.54], [157.55, 7.39, 120.6, -14.1], [158.27, 7.13, 122.6, -13.72], [161.43, 7.44, 124.24, -13.4], [161.48, 7.19, 125.54, -13.12], [161.78, 7.05, 126.55, -12.89], [161.18, 6.78, 127.28, -12.7], [160.69, 6.58, 127.77, -12.54]]\n",
      "[[161.57, 6.71, 128.02, -12.42], [161.06, 6.59, 128.09, -12.36], [161.37, 6.67, 128.0, -12.32], [161.61, 6.76, 127.8, -12.39], [160.74, 6.65, 127.51, -12.43], [159.61, 6.5, 127.12, -12.47], [158.94, 6.46, 126.63, -12.51], [158.28, 6.45, 126.01, -12.58], [159.08, 6.75, 125.33, -12.69], [157.81, 6.64, 124.61, -12.74]]\n",
      "[[155.43, 6.32, 123.82, -12.69], [154.62, 6.33, 122.98, -12.64], [152.9, 6.17, 122.06, -12.62], [152.09, 6.21, 121.06, -12.61], [151.06, 6.21, 120.01, -12.62], [149.98, 6.21, 118.91, -12.64], [148.43, 6.13, 117.75, -12.64], [146.43, 5.98, 116.54, -12.62], [144.98, 5.94, 115.27, -12.62], [144.46, 6.1, 113.95, -12.64]]\n",
      "[[142.44, 5.97, 112.59, -12.61], [140.24, 5.81, 111.19, -12.59], [138.53, 5.75, 109.76, -12.58], [136.78, 5.69, 108.31, -12.56], [135.22, 5.68, 106.83, -12.56], [134.16, 5.76, 105.34, -12.54], [132.76, 5.78, 103.87, -12.54], [131.28, 5.77, 102.4, -12.53], [132.98, 6.39, 101.01, -12.52], [132.6, 6.58, 99.68, -12.51]]\n",
      "[[129.52, 6.22, 98.42, -12.51], [127.25, 6.01, 97.21, -12.52], [125.23, 5.84, 96.03, -12.54], [123.8, 5.79, 94.86, -12.56], [123.04, 5.87, 93.69, -12.54], [121.12, 5.72, 92.52, -12.51], [120.51, 5.83, 91.38, -12.49], [119.36, 5.81, 90.31, -12.49], [118.06, 5.76, 89.28, -12.49], [116.61, 5.67, 88.27, -12.49]]\n",
      "[[115.09, 5.56, 87.27, -12.51], [113.97, 5.54, 86.28, -12.54], [113.31, 5.61, 85.26, -12.57], [112.16, 5.59, 84.23, -12.6], [112.28, 5.82, 83.19, -12.64], [116.67, 6.54, 83.96, -12.74], [125.69, 7.73, 87.06, -12.64], [128.4, 7.36, 91.58, -12.59], [132.82, 7.18, 96.91, -12.59], [139.37, 7.37, 102.53, -12.6]]\n",
      "[[143.77, 7.13, 108.14, -12.62], [148.4, 6.98, 113.52, -12.59], [152.97, 6.88, 118.55, -12.33], [156.79, 6.73, 123.14, -12.06], [162.08, 6.95, 127.34, -11.83], [165.33, 6.84, 131.13, -11.63], [167.81, 6.66, 134.51, -11.46], [172.41, 6.98, 137.52, -11.32], [174.97, 6.95, 140.21, -11.19], [176.11, 6.71, 142.56, -11.07]]\n",
      "[[185.02, 8.08, 144.61, -10.96], [184.42, 7.62, 146.33, -10.87], [182.52, 6.96, 147.74, -10.81], [182.8, 6.78, 148.89, -10.77], [182.82, 6.61, 149.78, -10.71], [183.07, 6.52, 150.44, -10.64], [182.43, 6.31, 150.89, -10.57], [182.23, 6.22, 151.12, -10.52], [181.3, 6.03, 151.15, -10.49], [180.81, 5.96, 151.01, -10.45]]\n",
      "[[179.92, 5.84, 150.7, -10.42], [179.08, 5.77, 150.23, -10.39], [179.16, 5.91, 149.63, -10.37], [177.49, 5.72, 148.91, -10.36], [176.42, 5.67, 148.07, -10.36], [175.56, 5.68, 147.14, -10.37], [174.07, 5.59, 146.12, -10.39], [173.82, 5.76, 145.01, -10.49], [172.55, 5.74, 143.84, -10.5], [171.41, 5.76, 142.63, -10.5]]\n",
      "[[170.36, 5.79, 141.4, -10.51], [169.41, 5.85, 140.15, -10.53], [168.5, 5.92, 138.91, -10.58], [167.11, 5.89, 137.68, -10.65], [165.82, 5.87, 136.45, -10.74], [163.89, 5.73, 135.23, -10.83], [162.11, 5.62, 134.0, -10.94], [160.26, 5.5, 132.77, -11.06], [158.54, 5.4, 131.52, -11.12], [156.89, 5.33, 130.25, -11.17]]\n",
      "[[155.25, 5.26, 128.95, -11.11], [153.61, 5.2, 127.62, -11.05], [152.33, 5.21, 126.28, -11.02], [151.17, 5.25, 124.9, -11.03], [149.25, 5.14, 123.53, -11.05], [147.92, 5.16, 122.14, -11.08], [146.52, 5.15, 120.77, -11.12], [145.25, 5.17, 119.42, -11.17], [143.51, 5.09, 118.08, -11.24], [142.04, 5.06, 116.75, -11.35]]\n",
      "[[140.76, 5.07, 115.43, -11.46], [140.13, 5.2, 114.11, -11.53], [138.53, 5.15, 112.79, -11.6], [136.26, 4.95, 111.49, -11.66], [135.29, 5.02, 110.19, -11.73], [134.66, 5.13, 109.0, -11.82], [133.61, 5.13, 107.97, -11.9], [132.73, 5.13, 107.07, -11.99], [132.12, 5.17, 106.25, -12.08], [130.3, 4.96, 105.47, -12.17]]\n",
      "[[129.54, 4.96, 104.73, -12.25], [128.43, 4.89, 103.99, -12.3], [127.43, 4.84, 103.22, -12.37], [126.74, 4.86, 102.45, -12.4], [125.68, 4.8, 101.68, -12.39], [125.15, 4.86, 100.85, -12.37], [124.05, 4.81, 100.01, -12.35], [123.51, 4.87, 99.15, -12.33], [123.9, 5.13, 98.27, -12.28], [121.94, 4.91, 97.4, -12.2]]\n",
      "[[120.7, 4.84, 96.51, -12.13], [119.51, 4.78, 95.6, -12.09], [121.25, 5.3, 94.76, -12.01], [120.57, 5.3, 94.06, -11.95], [120.8, 5.47, 93.45, -11.9], [119.86, 5.39, 92.92, -11.89], [122.39, 5.93, 92.76, -11.9], [122.94, 5.99, 93.01, -11.93], [123.49, 5.99, 93.54, -11.86], [123.1, 5.77, 94.26, -11.8]]\n",
      "[[122.53, 5.49, 95.05, -11.6], [122.38, 5.31, 95.83, -11.44], [122.42, 5.17, 96.55, -11.31], [124.25, 5.42, 97.15, -11.2], [123.39, 5.15, 97.62, -11.14], [123.43, 5.09, 97.96, -11.1], [123.5, 5.07, 98.15, -11.08], [122.9, 4.95, 98.17, -11.08], [122.76, 4.95, 98.02, -11.09], [122.88, 5.03, 97.74, -11.12]]\n",
      "[[122.11, 4.96, 97.3, -11.09], [122.08, 5.05, 96.83, -11.06], [122.01, 5.14, 96.3, -11.05], [121.84, 5.23, 95.71, -11.07], [122.37, 5.45, 95.12, -11.08], [121.55, 5.39, 94.61, -11.05], [120.87, 5.35, 94.12, -11.03], [121.04, 5.49, 93.61, -11.03], [119.72, 5.33, 93.09, -11.03], [118.32, 5.16, 92.55, -11.04]]\n",
      "[[117.43, 5.1, 91.95, -11.05], [116.45, 5.03, 91.3, -11.07], [115.35, 4.96, 90.57, -11.1], [114.51, 4.95, 89.77, -11.12], [113.24, 4.87, 88.9, -11.15], [111.53, 4.72, 87.95, -11.18], [110.24, 4.66, 86.93, -11.22], [109.01, 4.63, 85.84, -11.26], [108.02, 4.67, 84.69, -11.3], [106.64, 4.62, 83.53, -11.35]]\n",
      "[[104.83, 4.5, 82.32, -11.41], [104.04, 4.59, 81.09, -11.46], [102.63, 4.55, 79.86, -11.52], [117.73, 7.78, 78.83, -11.59], [113.73, 7.06, 78.46, -11.67], [111.77, 6.55, 79.01, -11.76], [114.66, 6.83, 80.5, -11.82], [115.85, 6.63, 82.7, -11.86], [117.79, 6.5, 85.29, -11.91], [124.82, 7.35, 88.04, -11.96]]\n",
      "[[124.85, 6.78, 90.93, -11.91], [125.96, 6.4, 93.98, -11.79], [127.75, 6.15, 97.01, -11.58], [129.77, 5.96, 99.97, -11.28], [131.77, 5.81, 102.73, -11.03], [133.69, 5.7, 105.21, -10.81], [135.43, 5.61, 107.35, -10.63], [140.48, 6.26, 109.19, -10.38], [141.3, 6.1, 110.8, -10.15], [142.05, 5.98, 112.15, -9.98]]\n",
      "[[143.14, 5.98, 113.24, -9.81], [143.98, 5.98, 114.07, -9.76], [147.84, 6.63, 114.71, -9.67], [146.94, 6.37, 115.12, -9.54], [146.44, 6.22, 115.33, -9.43], [145.35, 6.0, 115.34, -9.34], [144.58, 5.88, 115.16, -9.27], [143.99, 5.84, 114.81, -9.22], [142.71, 5.69, 114.28, -9.19], [144.63, 6.21, 113.56, -9.18]]\n",
      "[[143.75, 6.21, 112.72, -9.19], [143.99, 6.39, 112.03, -9.19], [142.89, 6.24, 111.67, -9.21], [143.14, 6.32, 111.51, -9.25], [141.67, 6.04, 111.46, -9.31], [141.33, 5.98, 111.43, -9.38], [140.9, 5.9, 111.37, -9.47], [139.91, 5.73, 111.25, -9.56], [139.1, 5.62, 111.02, -9.66], [139.7, 5.81, 110.67, -9.76]]\n",
      "[[139.4, 5.83, 110.24, -9.74], [138.71, 5.8, 109.69, -9.73], [137.84, 5.76, 109.03, -9.75], [137.18, 5.78, 108.26, -9.77], [135.94, 5.71, 107.37, -9.73], [134.83, 5.67, 106.45, -9.7], [133.61, 5.63, 105.47, -9.68], [131.7, 5.46, 104.41, -9.65], [138.28, 6.95, 103.54, -9.69], [135.88, 6.58, 102.98, -9.67]]\n",
      "[[134.69, 6.41, 102.66, -9.56], [134.29, 6.36, 102.48, -9.47], [133.14, 6.16, 102.36, -9.4], [131.75, 5.9, 102.24, -9.36], [131.08, 5.8, 102.08, -9.34], [130.28, 5.69, 101.83, -9.34], [134.12, 6.49, 101.66, -9.35], [135.28, 6.72, 101.66, -9.38], [135.08, 6.66, 101.75, -9.42], [134.3, 6.49, 101.86, -9.46]]\n",
      "[[134.2, 6.46, 101.91, -9.52], [133.09, 6.24, 101.88, -9.6], [131.47, 5.94, 101.75, -9.65], [130.65, 5.83, 101.49, -9.71], [130.24, 5.83, 101.09, -9.79], [128.81, 5.65, 100.56, -9.87], [127.68, 5.56, 99.9, -9.97], [126.29, 5.43, 99.11, -10.07], [124.59, 5.28, 98.2, -10.17], [124.29, 5.41, 97.26, -10.28]]\n",
      "[[122.85, 5.31, 96.29, -10.38], [122.07, 5.36, 95.27, -10.45], [120.76, 5.31, 94.2, -10.54], [126.14, 6.52, 93.52, -10.56], [128.93, 7.08, 93.54, -10.48], [130.19, 7.18, 94.27, -10.37], [135.96, 8.09, 95.52, -10.26], [138.24, 8.1, 97.74, -10.17], [140.53, 7.91, 100.97, -10.13], [143.69, 7.78, 104.78, -10.1]]\n",
      "[[146.55, 7.54, 108.84, -9.97], [149.9, 7.4, 112.9, -9.84], [151.99, 7.04, 116.76, -9.73], [153.01, 6.54, 120.32, -9.62], [155.4, 6.38, 123.51, -9.47], [157.22, 6.19, 126.3, -9.29], [158.69, 6.0, 128.67, -9.13], [159.96, 5.87, 130.61, -9.0], [161.32, 5.83, 132.16, -8.9], [161.76, 5.69, 133.32, -8.82]]\n",
      "[[162.87, 5.75, 134.1, -8.76], [164.27, 5.94, 134.58, -8.73], [165.08, 6.05, 134.85, -8.72], [164.87, 6.0, 134.9, -8.71], [164.47, 5.94, 134.75, -8.72], [163.78, 5.87, 134.41, -8.67], [162.79, 5.78, 133.88, -8.61], [162.71, 5.9, 133.22, -8.5], [162.04, 5.91, 132.46, -8.39], [160.6, 5.8, 131.59, -8.3]]\n",
      "[[160.6, 6.0, 130.61, -8.24], [159.91, 6.07, 129.56, -8.2], [158.57, 6.03, 128.43, -8.18], [156.31, 5.81, 127.25, -8.18], [154.67, 5.74, 125.98, -8.19], [154.15, 5.89, 124.69, -8.23], [152.6, 5.84, 123.42, -8.29], [150.91, 5.75, 122.14, -8.36], [149.04, 5.64, 120.83, -8.46], [147.17, 5.54, 119.48, -8.56]]\n",
      "[[145.92, 5.57, 118.07, -8.68], [143.63, 5.4, 116.61, -8.79], [141.49, 5.28, 115.09, -8.91], [139.51, 5.2, 113.51, -9.03], [137.8, 5.18, 111.88, -9.15], [137.71, 5.49, 110.27, -9.26], [136.62, 5.55, 108.84, -9.38], [138.06, 6.06, 107.75, -9.47], [136.82, 5.93, 107.15, -9.58], [138.44, 6.32, 106.85, -9.72]]\n",
      "[[137.2, 6.07, 106.87, -9.84], [137.57, 6.09, 107.12, -9.91], [137.11, 5.93, 107.48, -9.82], [138.91, 6.19, 107.96, -9.75], [137.86, 5.87, 108.5, -9.71], [137.68, 5.73, 109.03, -9.69], [137.44, 5.59, 109.49, -9.64], [137.53, 5.54, 109.83, -9.62], [138.92, 5.77, 110.07, -9.51], [139.0, 5.74, 110.31, -9.38]]\n",
      "[[138.43, 5.59, 110.47, -9.27], [138.39, 5.56, 110.57, -9.18], [139.84, 5.84, 110.62, -9.12], [139.42, 5.77, 110.58, -9.07], [139.88, 5.88, 110.47, -9.05], [139.13, 5.76, 110.3, -9.04], [137.98, 5.59, 110.05, -9.01], [137.15, 5.5, 109.66, -8.93], [135.9, 5.35, 109.17, -8.86], [135.02, 5.3, 108.54, -8.81]]\n",
      "[[134.17, 5.28, 107.79, -8.77], [133.16, 5.25, 106.89, -8.76], [131.39, 5.1, 105.87, -8.76], [129.77, 5.01, 104.72, -8.77], [128.01, 4.91, 103.46, -8.8], [126.37, 4.86, 102.1, -8.83], [124.75, 4.82, 100.64, -8.87], [122.91, 4.76, 99.11, -8.91], [120.98, 4.7, 97.49, -8.95], [119.23, 4.69, 95.78, -9.0]]\n",
      "[[117.01, 4.6, 94.03, -9.05], [115.1, 4.58, 92.21, -9.11], [112.92, 4.51, 90.36, -9.17], [113.0, 4.9, 88.49, -9.24], [110.53, 4.76, 86.73, -9.31], [109.05, 4.8, 85.05, -9.38], [106.67, 4.65, 83.41, -9.45], [105.13, 4.66, 81.83, -9.54], [102.89, 4.52, 80.28, -9.62], [100.74, 4.4, 78.74, -9.71]]\n",
      "[[98.99, 4.35, 77.22, -9.81], [97.53, 4.36, 75.72, -9.91], [95.59, 4.28, 74.22, -10.01], [99.28, 5.28, 72.9, -10.12], [97.61, 5.12, 71.98, -10.23], [99.28, 5.57, 71.41, -10.34], [101.86, 6.07, 71.52, -10.44], [102.19, 5.96, 72.41, -10.54], [101.76, 5.6, 73.77, -10.64], [103.26, 5.58, 75.37, -10.74]]\n",
      "[[104.82, 5.56, 77.04, -10.85], [129.82, 9.95, 80.09, -10.96], [128.66, 8.7, 85.16, -10.84], [132.79, 8.29, 91.32, -10.73], [137.56, 7.94, 97.88, -10.65], [146.56, 8.43, 104.43, -10.59], [149.06, 7.67, 110.72, -10.52], [152.24, 7.12, 116.62, -10.43], [155.92, 6.78, 122.03, -10.11], [159.6, 6.54, 126.92, -9.84]]\n",
      "[[163.31, 6.41, 131.26, -9.64], [166.66, 6.32, 135.04, -9.47], [169.5, 6.24, 138.3, -9.34], [172.55, 6.3, 141.06, -9.22], [174.69, 6.26, 143.39, -9.09], [179.18, 6.78, 145.3, -8.94], [178.94, 6.4, 146.91, -8.75], [182.4, 6.83, 148.24, -8.59], [180.36, 6.22, 149.27, -8.45], [181.14, 6.22, 150.03, -8.35]]\n",
      "[[181.35, 6.17, 150.52, -8.27], [181.01, 6.05, 150.75, -8.21], [180.79, 6.01, 150.75, -8.17], [180.05, 5.91, 150.52, -8.14], [179.3, 5.85, 150.07, -8.13], [178.21, 5.76, 149.43, -8.1], [177.03, 5.68, 148.62, -8.05], [175.51, 5.57, 147.66, -8.01], [174.06, 5.5, 146.55, -7.98], [172.16, 5.37, 145.31, -7.97]]\n",
      "[[170.37, 5.29, 143.94, -7.96], [168.64, 5.24, 142.46, -7.96], [169.04, 5.63, 140.91, -7.97], [167.88, 5.71, 139.32, -8.0], [166.11, 5.68, 137.71, -8.03], [164.51, 5.69, 136.07, -8.07], [161.74, 5.47, 134.41, -8.12], [159.33, 5.33, 132.7, -8.17], [157.06, 5.22, 130.94, -8.23], [154.69, 5.11, 129.15, -8.29]]\n",
      "[[152.52, 5.04, 127.31, -8.35], [150.86, 5.08, 125.48, -8.42], [149.65, 5.2, 123.66, -8.5], [153.93, 6.28, 122.52, -8.59], [159.65, 7.46, 122.36, -8.69], [158.05, 6.97, 123.2, -8.8], [158.05, 6.67, 124.7, -8.91], [159.48, 6.58, 126.57, -9.01], [161.08, 6.5, 128.6, -9.11], [161.68, 6.2, 130.66, -9.16]]\n",
      "[[163.76, 6.23, 132.63, -8.98], [164.69, 6.05, 134.45, -8.73], [166.51, 6.09, 136.05, -8.52], [168.26, 6.16, 137.44, -8.34], [168.7, 6.01, 138.66, -8.2], [169.51, 5.97, 139.67, -8.04], [169.76, 5.86, 140.45, -7.9], [169.75, 5.75, 141.0, -7.78], [169.46, 5.63, 141.32, -7.68], [169.81, 5.67, 141.44, -7.6]]\n",
      "[[181.63, 7.57, 143.76, -7.54], [187.82, 7.53, 150.18, -7.49], [194.78, 7.16, 158.99, -7.45], [203.98, 7.0, 168.96, -7.36], [213.8, 6.91, 179.22, -7.27], [225.77, 7.29, 189.31, -7.13], [234.27, 7.07, 198.9, -6.93], [243.08, 7.05, 207.85, -6.77], [250.62, 6.92, 216.04, -6.63], [257.8, 6.87, 223.45, -6.5]]\n",
      "[[266.11, 7.2, 230.12, -6.4], [270.67, 6.93, 236.01, -6.31], [276.3, 7.02, 241.22, -6.24], [280.65, 6.97, 245.8, -6.17], [284.03, 6.85, 249.78, -6.12], [286.81, 6.72, 253.21, -6.08], [289.19, 6.61, 256.13, -6.05], [291.08, 6.5, 258.56, -6.02], [292.52, 6.39, 260.56, -6.01], [293.44, 6.25, 262.17, -5.99]]\n",
      "[[294.5, 6.22, 263.4, -5.97], [295.21, 6.18, 264.32, -5.94], [295.5, 6.11, 264.94, -5.91], [295.58, 6.06, 265.29, -5.89], [295.47, 6.02, 265.39, -5.87], [295.12, 5.97, 265.27, -5.86], [294.91, 5.99, 264.96, -5.85], [294.22, 5.94, 264.51, -5.85], [294.16, 6.05, 263.9, -5.85], [293.58, 6.08, 263.17, -5.85]]\n",
      "[[292.57, 6.05, 262.31, -5.86], [291.37, 6.01, 261.34, -5.86], [293.24, 6.59, 260.27, -5.87], [292.68, 6.71, 259.14, -5.88], [291.09, 6.63, 257.94, -5.89], [289.7, 6.6, 256.68, -5.91], [287.5, 6.43, 255.34, -5.92], [285.48, 6.31, 253.94, -5.94], [283.69, 6.24, 252.48, -5.94], [281.89, 6.18, 250.98, -5.94]]\n",
      "[[280.04, 6.12, 249.44, -5.94], [278.16, 6.06, 247.86, -5.95], [276.71, 6.09, 246.24, -5.95], [274.85, 6.05, 244.59, -5.96], [272.98, 6.01, 242.92, -5.96], [270.95, 5.95, 241.21, -5.97], [269.07, 5.92, 239.49, -5.98], [267.24, 5.9, 237.74, -5.99], [264.85, 5.78, 235.97, -5.99], [262.59, 5.68, 234.19, -6.0]]\n",
      "[[260.65, 5.65, 232.4, -6.01], [258.47, 5.57, 230.59, -6.02], [256.04, 5.45, 228.78, -6.03], [254.41, 5.49, 226.96, -6.04], [252.5, 5.47, 225.14, -6.05], [250.51, 5.44, 223.31, -6.06], [248.52, 5.41, 221.49, -6.08], [246.44, 5.35, 219.67, -6.09], [244.4, 5.31, 217.85, -6.1], [242.33, 5.26, 216.02, -6.11]]\n",
      "[[240.21, 5.2, 214.2, -6.12], [238.24, 5.17, 212.38, -6.14], [236.62, 5.21, 210.56, -6.15], [234.57, 5.17, 208.74, -6.16], [232.51, 5.12, 206.92, -6.17], [230.24, 5.03, 205.11, -6.19], [228.45, 5.03, 203.3, -6.2], [226.26, 4.95, 201.49, -6.21], [224.34, 4.93, 199.7, -6.23], [222.42, 4.9, 197.91, -6.24]]\n",
      "[[220.62, 4.9, 196.13, -6.26], [218.7, 4.87, 194.35, -6.27], [216.91, 4.87, 192.57, -6.29], [215.19, 4.88, 190.81, -6.3], [213.32, 4.85, 189.06, -6.31], [211.45, 4.83, 187.32, -6.33], [209.57, 4.79, 185.6, -6.35], [208.04, 4.83, 183.88, -6.36], [205.91, 4.74, 182.19, -6.38], [204.03, 4.7, 180.5, -6.39]]\n",
      "[[202.4, 4.71, 178.83, -6.41], [200.58, 4.69, 177.15, -6.43], [198.66, 4.64, 175.48, -6.44], [196.88, 4.61, 173.81, -6.46], [198.21, 5.22, 172.14, -6.48], [196.91, 5.27, 170.54, -6.49], [195.62, 5.32, 169.0, -6.51], [194.61, 5.42, 167.53, -6.53], [192.7, 5.32, 166.1, -6.55], [191.85, 5.43, 164.72, -6.56]]\n",
      "[[190.05, 5.34, 163.35, -6.58], [188.03, 5.21, 161.99, -6.6], [187.87, 5.45, 160.64, -6.62], [185.78, 5.3, 159.3, -6.64], [183.62, 5.13, 157.97, -6.66], [182.0, 5.07, 156.63, -6.68], [180.29, 5.0, 155.29, -6.7], [178.6, 4.93, 153.94, -6.72], [176.87, 4.86, 152.58, -6.74], [175.51, 4.86, 151.23, -6.75]]\n",
      "[[173.96, 4.82, 149.88, -6.77], [172.75, 4.85, 148.53, -6.79], [171.95, 4.95, 147.18, -6.82], [171.28, 5.08, 145.86, -6.84], [170.1, 5.11, 144.55, -6.86], [168.54, 5.06, 143.25, -6.88], [167.22, 5.05, 141.96, -6.9], [167.3, 5.31, 140.73, -6.92], [167.44, 5.57, 139.6, -6.95], [166.18, 5.51, 138.62, -6.97]]\n",
      "[[164.65, 5.38, 137.75, -6.99], [163.59, 5.33, 136.95, -7.01], [162.23, 5.21, 136.2, -7.04], [161.53, 5.21, 135.48, -7.06], [161.7, 5.39, 134.76, -7.08], [160.55, 5.3, 134.04, -7.11], [159.36, 5.21, 133.32, -7.13], [158.46, 5.18, 132.57, -7.16], [157.64, 5.17, 131.78, -7.18], [156.13, 5.04, 130.95, -7.2]]\n",
      "[[154.9, 4.97, 130.07, -7.21], [153.71, 4.91, 129.15, -7.23], [152.44, 4.85, 128.2, -7.25], [150.95, 4.75, 127.2, -7.26], [149.56, 4.68, 126.17, -7.28], [148.17, 4.61, 125.11, -7.3], [147.13, 4.62, 124.01, -7.31], [146.13, 4.65, 122.88, -7.33], [144.4, 4.54, 121.72, -7.35], [142.77, 4.45, 120.54, -7.37]]\n",
      "[[141.86, 4.5, 119.37, -7.39], [140.17, 4.4, 118.18, -7.4], [138.68, 4.34, 116.98, -7.42], [137.31, 4.31, 115.78, -7.44], [136.43, 4.37, 114.56, -7.46], [136.18, 4.56, 113.4, -7.48], [134.3, 4.41, 112.26, -7.5], [134.77, 4.73, 111.14, -7.52], [133.67, 4.72, 110.07, -7.54], [132.15, 4.63, 109.02, -7.56]]\n",
      "[[131.78, 4.76, 108.0, -7.58], [130.79, 4.75, 107.05, -7.6], [129.24, 4.62, 106.13, -7.62], [128.43, 4.63, 105.26, -7.64], [126.2, 4.35, 104.43, -7.66], [124.83, 4.24, 103.63, -7.69], [124.01, 4.23, 102.84, -7.71], [128.29, 5.18, 102.37, -7.73], [126.94, 4.94, 102.24, -7.75], [128.33, 5.2, 102.32, -7.77]]\n",
      "[[128.17, 5.12, 102.56, -7.79], [129.57, 5.33, 102.9, -7.82], [128.88, 5.11, 103.31, -7.84], [131.93, 5.63, 103.79, -7.86], [131.8, 5.48, 104.41, -7.88], [132.63, 5.51, 105.07, -7.91], [133.59, 5.56, 105.79, -7.93], [133.78, 5.45, 106.52, -7.95], [137.81, 6.09, 107.34, -7.98], [138.87, 6.08, 108.46, -8.0]]\n",
      "[[139.87, 6.03, 109.72, -8.02], [141.03, 5.99, 111.05, -8.05], [141.55, 5.83, 112.43, -8.07], [143.91, 6.03, 113.77, -8.09], [144.27, 5.85, 115.03, -8.11], [144.36, 5.65, 116.14, -8.13], [144.78, 5.54, 117.07, -8.16], [145.25, 5.48, 117.83, -8.18], [145.69, 5.46, 118.39, -8.2], [146.08, 5.47, 118.76, -8.22]]\n",
      "[[145.98, 5.41, 118.94, -8.24], [145.19, 5.25, 118.96, -8.27], [144.66, 5.17, 118.8, -8.29], [143.69, 5.04, 118.49, -8.31], [143.21, 5.04, 118.02, -8.33], [142.4, 4.99, 117.43, -8.35], [142.08, 5.07, 116.73, -8.37], [142.86, 5.37, 115.99, -8.4], [141.88, 5.33, 115.25, -8.42], [140.3, 5.16, 114.5, -8.44]]\n",
      "[[139.89, 5.23, 113.75, -8.46], [140.59, 5.52, 112.99, -8.48], [142.16, 5.97, 112.3, -8.5], [140.53, 5.77, 111.66, -8.52], [139.06, 5.59, 111.09, -8.54], [139.07, 5.7, 110.56, -8.56], [137.62, 5.51, 110.07, -8.58], [136.91, 5.46, 109.6, -8.6], [135.94, 5.37, 109.11, -8.61], [135.39, 5.36, 108.58, -8.63]]\n",
      "[[134.05, 5.21, 107.99, -8.65], [133.18, 5.16, 107.37, -8.67], [132.16, 5.09, 106.68, -8.69], [131.41, 5.1, 105.92, -8.71], [132.05, 5.39, 105.08, -8.73], [130.49, 5.26, 104.21, -8.75], [130.26, 5.4, 103.28, -8.77], [132.06, 5.94, 102.36, -8.79], [138.13, 7.13, 102.48, -8.82], [138.79, 6.96, 104.0, -8.84]]\n",
      "[[144.24, 7.53, 106.61, -8.87], [146.99, 7.38, 110.11, -8.89], [149.07, 7.0, 114.06, -8.92], [153.37, 7.04, 118.16, -8.94], [156.13, 6.79, 122.19, -8.96], [158.86, 6.57, 126.01, -8.83], [161.76, 6.45, 129.53, -8.6], [164.34, 6.34, 132.66, -8.33], [171.68, 7.24, 135.47, -8.12], [171.87, 6.78, 137.95, -7.94]]\n",
      "[[173.43, 6.66, 140.11, -7.83], [176.09, 6.81, 142.05, -7.69], [179.15, 7.06, 143.83, -7.57], [180.52, 7.0, 145.52, -7.49], [182.48, 7.07, 147.13, -7.42], [182.66, 6.82, 148.58, -7.36], [183.74, 6.77, 149.87, -7.32], [184.0, 6.59, 151.04, -7.3], [184.47, 6.48, 152.07, -7.28], [185.19, 6.45, 152.94, -7.21]]\n",
      "[[185.26, 6.32, 153.64, -7.0], [184.71, 6.11, 154.16, -6.82], [186.69, 6.44, 154.5, -6.66], [186.25, 6.31, 154.71, -6.53], [186.9, 6.42, 154.8, -6.43], [186.1, 6.27, 154.74, -6.33], [185.91, 6.27, 154.54, -6.24], [184.8, 6.12, 154.21, -6.13], [184.42, 6.14, 153.73, -6.05], [183.98, 6.17, 153.14, -5.99]]\n",
      "[[182.21, 5.96, 152.42, -5.95], [180.85, 5.85, 151.58, -5.93], [179.98, 5.87, 150.63, -5.91], [177.95, 5.68, 149.58, -5.89], [176.54, 5.63, 148.41, -5.89], [175.02, 5.57, 147.16, -5.9], [173.49, 5.54, 145.81, -5.92], [171.95, 5.51, 144.39, -5.95], [170.35, 5.49, 142.89, -6.0], [169.04, 5.54, 141.33, -6.05]]\n",
      "[[167.77, 5.61, 139.72, -6.11], [166.32, 5.64, 138.12, -6.17], [164.25, 5.54, 136.53, -6.25], [162.34, 5.48, 134.94, -6.33], [160.41, 5.42, 133.32, -6.41], [158.81, 5.42, 131.7, -6.47], [157.4, 5.47, 130.07, -6.48], [155.69, 5.45, 128.42, -6.5], [153.85, 5.42, 126.76, -6.53], [151.74, 5.33, 125.07, -6.57]]\n",
      "[[150.1, 5.35, 123.35, -6.61], [147.74, 5.23, 121.62, -6.65], [145.67, 5.16, 119.86, -6.7], [143.82, 5.15, 118.09, -6.76], [142.18, 5.18, 116.3, -6.81], [140.39, 5.18, 114.51, -6.87], [138.35, 5.13, 112.71, -6.93], [136.95, 5.21, 110.92, -7.0], [134.49, 5.07, 109.13, -7.07], [132.22, 4.98, 107.33, -7.15]]\n",
      "[[130.13, 4.92, 105.54, -7.22], [129.6, 5.15, 103.86, -7.3], [129.46, 5.43, 102.29, -7.39], [129.49, 5.74, 100.81, -7.49], [131.21, 6.36, 99.4, -7.56], [129.71, 6.31, 98.16, -7.61], [130.27, 6.64, 97.08, -7.57], [128.16, 6.41, 96.14, -7.62], [125.53, 6.05, 95.28, -7.66], [123.43, 5.79, 94.48, -7.72]]\n",
      "[[123.18, 5.89, 93.72, -7.78], [121.4, 5.68, 93.02, -7.89], [121.11, 5.75, 92.35, -8.02], [120.3, 5.71, 91.73, -8.13], [129.68, 7.59, 91.74, -8.11], [124.85, 6.48, 92.44, -8.12], [125.28, 6.33, 93.61, -8.13], [125.28, 6.04, 95.07, -8.15], [126.89, 6.05, 96.65, -8.18], [127.74, 5.91, 98.21, -8.23]]\n",
      "[[128.57, 5.78, 99.67, -8.28], [129.92, 5.78, 101.0, -8.32], [131.47, 5.87, 102.13, -8.35], [131.54, 5.7, 103.05, -8.39], [131.39, 5.52, 103.76, -8.42], [133.42, 5.82, 104.33, -8.46], [133.47, 5.72, 104.85, -8.49], [133.68, 5.68, 105.28, -8.51], [133.79, 5.64, 105.59, -8.55], [133.17, 5.49, 105.73, -8.6]]\n",
      "[[132.57, 5.37, 105.72, -8.67], [131.8, 5.25, 105.55, -8.75], [131.17, 5.19, 105.22, -8.84], [129.84, 5.02, 104.75, -8.95], [129.13, 5.0, 104.12, -9.05], [127.71, 4.87, 103.36, -9.09], [126.35, 4.78, 102.46, -9.14], [124.5, 4.61, 101.43, -9.18], [123.16, 4.58, 100.28, -9.2], [122.68, 4.72, 99.06, -9.22]]\n",
      "[[121.77, 4.8, 97.77, -9.25], [120.54, 4.82, 96.43, -9.28], [118.89, 4.77, 95.02, -9.31], [116.93, 4.67, 93.57, -9.34], [116.01, 4.79, 92.06, -9.37], [113.94, 4.69, 90.52, -9.4], [114.79, 5.15, 89.04, -9.43], [114.59, 5.37, 87.75, -9.46], [113.36, 5.35, 86.63, -9.5], [112.42, 5.34, 85.71, -9.54]]\n"
     ]
    }
   ],
   "source": [
    "noise = torch.zeros_like(audio)\n",
    "noise.requires_grad = True\n",
    "\n",
    "lr = 1e-1\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "reg_weight = 1\n",
    "ctc_weight = 5\n",
    "eps = 10\n",
    "\n",
    "losses = []\n",
    "min_loss = 100000\n",
    "min_noise = None\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss, ctc_loss_value, noise_reg, dB_x_delta = loss_function(audio, noise, target_logits, model, reg_weight, ctc_weight, eps)\n",
    "    if loss is None:\n",
    "        break\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    itemized_loss = loss.item()\n",
    "    losses_to_observe = [round(itemized_loss,2), round(ctc_loss_value,2), round(noise_reg,2), round(dB_x_delta,2)]\n",
    "    losses.append(losses_to_observe)\n",
    "    if itemized_loss < min_loss:\n",
    "        min_loss = itemized_loss\n",
    "        min_noise = noise.detach().cpu().numpy()\n",
    "    if i % 10 == 0:\n",
    "        print(losses[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THER TRUCOVERISI ANSO IRIDECASUUIAGOAN ATO ON DESVAO']\n"
     ]
    }
   ],
   "source": [
    "#test = audio + torch.tensor(min_noise).to(device)\n",
    "test = audio + noise\n",
    "logits = model(test).logits\n",
    "#print predicted sentence\n",
    "print(processor.batch_decode(torch.argmax(logits, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"test.wav\", test.detach().cpu(), sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERYTHING BELOW IS TRASH DO NOT BOTHER WITH IT\n",
    "\n",
    "unless you need inspiration or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = ds[0][\"text\"]\n",
    "target_sentence = [c for c in target_sentence]\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target_sentence)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "test_target_sentence = ds[5]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "softmaxed_logits = softmax(output_logits)\n",
    "\n",
    "# show predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(softmaxed_logits, dim=-1))\n",
    "print(predicted_sentence)\n",
    "print(ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.7382, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss(output_logits.unsqueeze(1), target_logits.unsqueeze(0), torch.tensor([output_logits.shape[0]]), torch.tensor([target_logits.shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[89]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[target_logits.shape[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "output_logits = model(audio).logits\n",
    "output_logits = softmax(output_logits[0])\n",
    "\n",
    "# get predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(output_logits, dim=-1))\n",
    "print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1263.2513, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_target_sentence = ds[1]['text']\n",
    "test_target_sentence = ds[2]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)\n",
    "\n",
    "ctc_loss(output_logits, test_target_logits.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"]['sampling_rate']).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.Tensor(ds[0][\"audio\"][\"array\"]).unsqueeze(0)\n",
    "rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "torchaudio.save(\"test.wav\", audio, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_transcription = 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n",
    "\n",
    "transcription = \"THE CAT JUMPED OVER THE FOX WHERE IT HAS SHOWN US THE WORLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = processor.tokenizer.convert_tokens_to_ids([c for c in transcription[0]])\n",
    "target = torch.Tensor(target).unsqueeze(0).long()\n",
    "target_shape = target[0].shape[0]\n",
    "logits_shape = logits[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-170.5979, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctcloss(log_probs=softmax(logits[0]), targets=target[0], input_lengths=[logits_shape], target_lengths=[target_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_characters = processor.tokenizer.convert_ids_to_tokens(predicted_ids[0].tolist())\n",
    "predicted_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [c for c in transcription[0]]\n",
    "transcription_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.convert_tokens_to_ids(transcription_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# remove consecutive duplicates\n",
    "result = [k for k, g in itertools.groupby(predicted_ids[0])]\n",
    "# remove blanks\n",
    "result = [x for x in result if x != 0]\n",
    "#count\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_ids>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert transcription to logits\n",
    "transcription_logits = processor(transcription, return_tensors=\"pt\", padding=\"longest\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files extra2a, extra2b and infer\n",
    "files = [\"extra2a.wav\", \"extra2b.wav\"]\n",
    "# read audios\n",
    "audio, rate = torchaudio.load(files[0])\n",
    "audio2, rate2 = torchaudio.load(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer audio1\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THAT DAY THE MERCHANT GAVE THE BOY PERMISSION TO BUILD THE DIS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer audio1\n",
    "file = \"extra_0a.wav\"\n",
    "audio, rate = torchaudio.load(file)\n",
    "audio = audio\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Minimize \n",
    "\n",
    "$ dB_x(\\delta) + c l(x+\\delta, t) $\n",
    "\n",
    "where \n",
    "\n",
    "$dB_x(\\delta)$ is the strength of the noise compared to the signal\n",
    "\n",
    "$c$: tradeoff parameter between being adversarial and being close to the original signal\n",
    "\n",
    "$l(x+\\delta, t)$ : the loss between the (disturbed signal prediction?) and the target sentence to become adversarial t?\n",
    "\n",
    "we define $l$ as the CTC-loss, so we can say:\n",
    "\n",
    "$-log Pr(t | x+\\delta) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"]\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target[0]]\n",
    "# convert to tensor logits\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#compute adversarial example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "audio = processor(audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values\n",
    "audio = audio[0].to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize dB_x(delta) - logPr(t|f(x+delta)), such that dB_x(delta) < eps\n",
    "# delta = argmin dB_x(delta) - logPr(t|f(x+delta))\n",
    "# dB_x(delta) = 20*log10(||x+delta||_2 / ||delta||_2)\n",
    "# x = audio\n",
    "# t = target transcription\n",
    "# f = model\n",
    "# eps = max distortion\n",
    "# delta = perturbation\n",
    "\n",
    "\n",
    "# define loss function\n",
    "def loss_function(audio, noise, target_logits, model, eps, ctc_constant):\n",
    "    audio_perturbed = audio + noise\n",
    "    # print(input_values.shape)\n",
    "    # print(audio_perturbed.shape)\n",
    "    #audio: clean audio\n",
    "    # calculate dB_x, dB_delta, dB_x(delta) , where delta is perturbed_noise - clean_audio\n",
    "    dB_x = 20*torch.log10(torch.norm(audio))\n",
    "    # calculate dB_delta\n",
    "    # add 1e-10 to avoid log of zero\n",
    "    dB_delta = 20*torch.log10(torch.norm(noise+1e-10))\n",
    "    # calculate dB_x(delta)\n",
    "    dB_x_delta = dB_delta - dB_x\n",
    "\n",
    "    # calculate logPr(t|f(x+delta))\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = logits[0] # remove batch dimension\n",
    "    logits = softmax(logits)\n",
    "    # print(target_logits)\n",
    "    # print(target_logits.shape, target_logits.dtype)\n",
    "    # print(logits)\n",
    "    # print(logits.shape, logits.dtype)\n",
    "\n",
    "    # print(logits.shape[0])\n",
    "    # print(target_logits.shape[0])\n",
    "    # print(target_logits)\n",
    "    # print(logits)\n",
    "    logPr = ctcloss(logits, target_logits, [logits.shape[0]], [target_logits.shape[0]])\n",
    "    # calculate loss\n",
    "    # print(\"dB_x_delta, logPr\")\n",
    "    # print(dB_x_delta, logPr)\n",
    "    # loss = dB_x_delta - ctc_constant * logPr\n",
    "    loss = - logPr\n",
    "\n",
    "    # check if dbloss is smaller than eps\n",
    "    if dB_x_delta < eps:\n",
    "        return loss\n",
    "    else:\n",
    "        # print(dB_x_delta, eps)\n",
    "        # return None\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss\n",
      "tensor(5.8390, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "final loss\n",
      "tensor(5.9684, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctc_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctc_constant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# break if loss is None\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[25], line 38\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(audio, noise, target_logits, model, eps, ctc_constant)\u001b[0m\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(target_logits.shape, target_logits.dtype)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m logPr \u001b[38;5;241m=\u001b[39m \u001b[43mctcloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"dB_x_delta, logPr\")\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# print(dB_x_delta, logPr)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# loss = dB_x_delta - ctc_constant * logPr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m logPr\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1770\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\functional.py:2656\u001b[0m, in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[0;32m   2650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2651\u001b[0m         ctc_loss,\n\u001b[0;32m   2652\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   2653\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[0;32m   2654\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[0;32m   2655\u001b[0m     )\n\u001b[1;32m-> 2656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define eps\n",
    "eps = 10\n",
    "# define number of iterations\n",
    "n = 5000\n",
    "# define learning rate\n",
    "lr = 1e-1\n",
    "# define perturbed audio: start with clean audio\n",
    "noise = torch.zeros_like(audio).requires_grad_(True)\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "ctc_constant = 1\n",
    "\n",
    "# loop over n iterations\n",
    "for i in range(n):\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # calculate loss\n",
    "    loss = loss_function(audio, noise, target_logits, model, eps, ctc_constant=ctc_constant)\n",
    "    # break if loss is None\n",
    "    if loss is None:\n",
    "        break\n",
    "    print(\"final loss\")\n",
    "    print(loss)      \n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update perturbation\n",
    "    optimizer.step()\n",
    "#    print(audio_pert)\n",
    "# save adversarial example\n",
    "audio_pert = (audio+noise).detach().to(\"cpu\")\n",
    "torchaudio.save(\"adversarial_one.wav\", audio_pert, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.0689, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3960265 ,  0.37243792, -0.37965736, ...,  0.33345932,\n",
       "        -0.42485094,  0.32958943]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3785, -0.3786, -0.3785,  ..., -0.4242, -0.4231, -0.4313]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display audio object\n",
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.0002,  0.0002,  ..., -0.0454, -0.0443, -0.0526]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxed = torch.nn.Softmax(dim=1)\n",
    "probs = softmaxed(model(perturbed_audio).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"THE WILL BURN YOU TO A CRISP\"\n",
    "#convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_audio = audio + audio_pert\n",
    "dB_x = 20 * torch.log10(torch.norm(audio) / torch.norm(audio_pert))\n",
    "# calculate logPr(t|f(x+delta))\n",
    "logits = model(perturbed_audio).logits\n",
    "pred = processor.batch_decode(torch.argmax(logits, dim=-1))\n",
    "\n",
    "#print(target)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
