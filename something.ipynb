{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# libary to read audio in torch\n",
    "import torchaudio\n",
    "from torch.nn import CTCLoss\n",
    "from jiwer import wer\n",
    "import soundfile as sf\n",
    "\n",
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "ctcloss = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c81c9fc806f4af28d416107d8900e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa2b0d1605b46e89bba8d75ca7128f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6597d5818251467690f100ce68646e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e2908e35f749cfb9322d2554f5a55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6e07f396134800ac81d5a4c9a96b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fb9bdef04a41ba96b70812f0c9e506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# example of tokenizing an audio file\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select an audio file and process it\n",
    "#question: is it possible to process the audio after adding noise to it and backpropagate the loss?\n",
    "audio = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values\n",
    "sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "#sentence we want our model to predict\n",
    "target = \"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target]\n",
    "# convert to tensor logits using the tokenizer\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#load everything to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(logits, targets):\n",
    "    #this function only tested for batch_size = 1\n",
    "    input_lengths = torch.tensor([logits.shape[0]])\n",
    "    target_lengths = torch.tensor([targets.shape[0]])\n",
    "    return ctcloss(logits, targets, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(audio, noise, target_logits, model):\n",
    "    audio_perturbed = audio + noise\n",
    "    #compute dB_x\n",
    "    dB_x = \n",
    "    #compute dB_delta\n",
    "    ...\n",
    "    #compute dB_x_delta\n",
    "    ...\n",
    "    #compute logits\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = softmax(logits[0])\n",
    "    logits = logits.unsqueeze(1)\n",
    "    #compute ctc loss\n",
    "    loss = ctc_loss(logits, target_logits)\n",
    "    #compute total loss\n",
    "    ...\n",
    "    loss = loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27.828081130981445]\n",
      "[24.386919021606445, 13.813969612121582, 11.216731071472168, 9.805255889892578, 9.317750930786133, 8.759771347045898, 9.356657981872559, 8.448748588562012, 8.402827262878418, 8.880524635314941]\n",
      "[8.306851387023926, 7.964898586273193, 7.836520195007324, 7.748620986938477, 7.669915676116943, 7.517343521118164, 7.5303778648376465, 7.535662651062012, 7.7495551109313965, 7.684903144836426]\n",
      "[7.48953104019165, 7.380371570587158, 7.442655086517334, 7.3777594566345215, 7.167582988739014, 7.004438400268555, 6.784716606140137, 6.724461555480957, 6.5612335205078125, 6.5858235359191895]\n",
      "[6.571091175079346, 6.461272716522217, 6.3936028480529785, 6.291732311248779, 6.27535343170166, 6.24393367767334, 6.211443901062012, 6.226598739624023, 6.445048809051514, 6.444520473480225]\n",
      "[6.476072311401367, 6.417241096496582, 6.378167152404785, 6.2178144454956055, 6.142199516296387, 6.031375885009766, 5.951697826385498, 5.912008285522461, 5.873651027679443, 5.761438369750977]\n",
      "[5.689579486846924, 5.596868991851807, 5.545788764953613, 5.5122857093811035, 5.499186992645264, 5.412187576293945, 5.383224964141846, 5.3501787185668945, 5.314876079559326, 5.263836860656738]\n",
      "[5.204726696014404, 5.1672186851501465, 5.135838031768799, 5.077291488647461, 5.047705173492432, 5.011540412902832, 4.969170570373535, 4.950429916381836, 4.924758434295654, 4.903846263885498]\n",
      "[4.838607311248779, 4.837287902832031, 4.796905040740967, 4.9024457931518555, 4.894923210144043, 5.053204536437988, 4.961256504058838, 4.951616287231445, 4.951846122741699, 4.907726764678955]\n",
      "[4.82187032699585, 4.733325004577637, 4.711899280548096, 4.705176830291748, 4.655790328979492, 4.580731391906738, 4.566325664520264, 4.548593044281006, 4.770593166351318, 4.9332709312438965]\n",
      "[4.925420761108398, 4.895665168762207, 4.855438709259033, 4.874345779418945, 4.748918056488037, 4.713860034942627, 4.6716718673706055, 4.72067403793335, 4.588749408721924, 4.6084513664245605]\n",
      "[4.6244916915893555, 4.5893473625183105, 4.545299053192139, 4.481903076171875, 4.417893409729004, 4.394158363342285, 4.451565742492676, 4.43205451965332, 4.440192699432373, 4.445921421051025]\n",
      "[4.3947858810424805, 4.377170085906982, 4.32610559463501, 4.309174060821533, 4.2388916015625, 4.152564525604248, 4.33245849609375, 4.395615577697754, 4.214060306549072, 4.221676826477051]\n",
      "[4.252213478088379, 4.10707426071167, 4.094975471496582, 4.021894931793213, 3.994370460510254, 3.953874111175537, 3.897991418838501, 3.880676031112671, 3.8396494388580322, 3.8256359100341797]\n",
      "[3.8250200748443604, 3.7521543502807617, 3.709235429763794, 3.6703603267669678, 3.6254818439483643, 3.5893044471740723, 3.524142265319824, 3.5071990489959717, 3.462327003479004, 3.4364876747131348]\n",
      "[3.510009527206421, 3.4614193439483643, 3.4243152141571045, 3.3496925830841064, 3.304133892059326, 3.291663885116577, 3.282588481903076, 3.202284336090088, 3.181602716445923, 3.148007392883301]\n",
      "[3.107532024383545, 3.051851749420166, 3.0321576595306396, 3.0858347415924072, 2.9769492149353027, 2.9450762271881104, 2.9397945404052734, 2.8555519580841064, 2.80580472946167, 2.801774024963379]\n",
      "[2.7816078662872314, 2.726888418197632, 2.745035409927368, 2.9725046157836914, 2.8071296215057373, 2.66975736618042, 2.570521831512451, 2.5614380836486816, 2.620932102203369, 2.5782535076141357]\n",
      "[2.944579839706421, 2.534400463104248, 2.662914514541626, 2.599851131439209, 2.7131404876708984, 2.875636100769043, 2.758021354675293, 2.7055823802948, 2.9127118587493896, 2.7233829498291016]\n",
      "[2.5663652420043945, 2.5505950450897217, 2.408459424972534, 2.3388471603393555, 2.2754416465759277, 2.245457887649536, 2.167086362838745, 2.168294668197632, 2.787672519683838, 2.5514702796936035]\n",
      "[2.622605800628662, 2.4264562129974365, 2.3081870079040527, 2.2465434074401855, 2.220341205596924, 2.252215623855591, 2.246609926223755, 2.1100411415100098, 2.0121145248413086, 1.9380396604537964]\n",
      "[1.9029077291488647, 1.7793927192687988, 1.717483401298523, 1.6655536890029907, 1.615638256072998, 1.536532998085022, 1.5060501098632812, 1.463748574256897, 1.4447418451309204, 1.3382655382156372]\n",
      "[1.3012795448303223, 1.2778370380401611, 1.2195290327072144, 1.175624132156372, 1.1027207374572754, 1.0742188692092896, 1.0414973497390747, 0.9947590827941895, 0.9623149633407593, 0.9222171902656555]\n",
      "[0.9066778421401978, 0.9042864441871643, 0.8807212710380554, 0.8795689344406128, 0.8494395613670349, 0.8303748965263367, 0.811535656452179, 0.7938060760498047, 0.7858417630195618, 0.7657233476638794]\n",
      "[0.7556092739105225, 0.74653160572052, 0.7776516675949097, 0.7572380900382996, 0.7718923687934875, 0.7528138160705566, 0.7594130635261536, 0.7390167117118835, 0.7375472784042358, 0.882149338722229]\n",
      "[0.8747103214263916, 0.908425509929657, 0.84531569480896, 0.9454473853111267, 1.038020372390747, 1.1151041984558105, 0.9570276737213135, 0.9282065033912659, 1.0374865531921387, 0.9694266319274902]\n",
      "[0.9909785985946655, 0.9709031581878662, 1.1240880489349365, 1.435726284980774, 1.992986798286438, 1.773200511932373, 1.5626819133758545, 1.5891038179397583, 1.5823980569839478, 1.4388031959533691]\n",
      "[1.3738752603530884, 1.2526806592941284, 1.2817193269729614, 1.2547036409378052, 1.1800892353057861, 1.1632733345031738, 1.04379403591156, 1.0230414867401123, 0.9438841342926025, 0.9091732501983643]\n",
      "[0.8385024070739746, 0.7870414853096008, 0.7651304006576538, 0.7545122504234314, 0.7333344221115112, 0.7086731791496277, 0.6819815635681152, 0.6631017327308655, 0.6468915343284607, 0.6318790912628174]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 15\u001b[0m itemized_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(itemized_loss)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m itemized_loss \u001b[38;5;241m<\u001b[39m min_loss:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "noise = torch.zeros_like(audio)\n",
    "noise.requires_grad = True\n",
    "\n",
    "lr = 1e-1\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "\n",
    "losses = []\n",
    "min_loss = 100000\n",
    "min_noise = None\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(audio, noise, target_logits, model)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    itemized_loss = loss.item()\n",
    "    losses.append(itemized_loss)\n",
    "    if itemized_loss < min_loss:\n",
    "        min_loss = itemized_loss\n",
    "        min_noise = noise.detach().cpu().numpy()\n",
    "    if i % 10 == 0:\n",
    "        print(losses[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE CAT<unk>IS<unk>INSIDE<unk>MY<unk>BAG<unk>AND<unk>IT<unk>ROLLS</s>ON<unk>THE<unk>FLOOR']\n"
     ]
    }
   ],
   "source": [
    "#test = audio + torch.tensor(min_noise).to(device)\n",
    "test = audio + noise\n",
    "logits = model(test).logits\n",
    "#print predicted sentence\n",
    "print(processor.batch_decode(torch.argmax(logits, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"test.wav\", test.detach().cpu(), sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERYTHING BELOW IS TRASH DO NOT BOTHER WITH IT\n",
    "\n",
    "unless you need inspiration or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = ds[0][\"text\"]\n",
    "target_sentence = [c for c in target_sentence]\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target_sentence)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "test_target_sentence = ds[5]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "softmaxed_logits = softmax(output_logits)\n",
    "\n",
    "# show predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(softmaxed_logits, dim=-1))\n",
    "print(predicted_sentence)\n",
    "print(ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.7382, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss(output_logits.unsqueeze(1), target_logits.unsqueeze(0), torch.tensor([output_logits.shape[0]]), torch.tensor([target_logits.shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[89]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[target_logits.shape[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "output_logits = model(audio).logits\n",
    "output_logits = softmax(output_logits[0])\n",
    "\n",
    "# get predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(output_logits, dim=-1))\n",
    "print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1263.2513, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_target_sentence = ds[1]['text']\n",
    "test_target_sentence = ds[2]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)\n",
    "\n",
    "ctc_loss(output_logits, test_target_logits.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"]['sampling_rate']).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.Tensor(ds[0][\"audio\"][\"array\"]).unsqueeze(0)\n",
    "rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "torchaudio.save(\"test.wav\", audio, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_transcription = 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n",
    "\n",
    "transcription = \"THE CAT JUMPED OVER THE FOX WHERE IT HAS SHOWN US THE WORLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = processor.tokenizer.convert_tokens_to_ids([c for c in transcription[0]])\n",
    "target = torch.Tensor(target).unsqueeze(0).long()\n",
    "target_shape = target[0].shape[0]\n",
    "logits_shape = logits[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-170.5979, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctcloss(log_probs=softmax(logits[0]), targets=target[0], input_lengths=[logits_shape], target_lengths=[target_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_characters = processor.tokenizer.convert_ids_to_tokens(predicted_ids[0].tolist())\n",
    "predicted_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [c for c in transcription[0]]\n",
    "transcription_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.convert_tokens_to_ids(transcription_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# remove consecutive duplicates\n",
    "result = [k for k, g in itertools.groupby(predicted_ids[0])]\n",
    "# remove blanks\n",
    "result = [x for x in result if x != 0]\n",
    "#count\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_ids>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert transcription to logits\n",
    "transcription_logits = processor(transcription, return_tensors=\"pt\", padding=\"longest\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files extra2a, extra2b and infer\n",
    "files = [\"extra2a.wav\", \"extra2b.wav\"]\n",
    "# read audios\n",
    "audio, rate = torchaudio.load(files[0])\n",
    "audio2, rate2 = torchaudio.load(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer audio1\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THAT DAY THE MERCHANT GAVE THE BOY PERMISSION TO BUILD THE DIS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer audio1\n",
    "file = \"extra_0a.wav\"\n",
    "audio, rate = torchaudio.load(file)\n",
    "audio = audio\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Minimize \n",
    "\n",
    "$ dB_x(\\delta) + c l(x+\\delta, t) $\n",
    "\n",
    "where \n",
    "\n",
    "$dB_x(\\delta)$ is the strength of the noise compared to the signal\n",
    "\n",
    "$c$: tradeoff parameter between being adversarial and being close to the original signal\n",
    "\n",
    "$l(x+\\delta, t)$ : the loss between the (disturbed signal prediction?) and the target sentence to become adversarial t?\n",
    "\n",
    "we define $l$ as the CTC-loss, so we can say:\n",
    "\n",
    "$-log Pr(t | x+\\delta) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"]\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target[0]]\n",
    "# convert to tensor logits\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#compute adversarial example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "audio = processor(audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values\n",
    "audio = audio[0].to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize dB_x(delta) - logPr(t|f(x+delta)), such that dB_x(delta) < eps\n",
    "# delta = argmin dB_x(delta) - logPr(t|f(x+delta))\n",
    "# dB_x(delta) = 20*log10(||x+delta||_2 / ||delta||_2)\n",
    "# x = audio\n",
    "# t = target transcription\n",
    "# f = model\n",
    "# eps = max distortion\n",
    "# delta = perturbation\n",
    "\n",
    "\n",
    "# define loss function\n",
    "def loss_function(audio, noise, target_logits, model, eps, ctc_constant):\n",
    "    audio_perturbed = audio + noise\n",
    "    # print(input_values.shape)\n",
    "    # print(audio_perturbed.shape)\n",
    "    #audio: clean audio\n",
    "    # calculate dB_x, dB_delta, dB_x(delta) , where delta is perturbed_noise - clean_audio\n",
    "    dB_x = 20*torch.log10(torch.norm(audio))\n",
    "    # calculate dB_delta\n",
    "    # add 1e-10 to avoid log of zero\n",
    "    dB_delta = 20*torch.log10(torch.norm(noise+1e-10))\n",
    "    # calculate dB_x(delta)\n",
    "    dB_x_delta = dB_delta - dB_x\n",
    "\n",
    "    # calculate logPr(t|f(x+delta))\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = logits[0] # remove batch dimension\n",
    "    logits = softmax(logits)\n",
    "    # print(target_logits)\n",
    "    # print(target_logits.shape, target_logits.dtype)\n",
    "    # print(logits)\n",
    "    # print(logits.shape, logits.dtype)\n",
    "\n",
    "    # print(logits.shape[0])\n",
    "    # print(target_logits.shape[0])\n",
    "    # print(target_logits)\n",
    "    # print(logits)\n",
    "    logPr = ctcloss(logits, target_logits, [logits.shape[0]], [target_logits.shape[0]])\n",
    "    # calculate loss\n",
    "    # print(\"dB_x_delta, logPr\")\n",
    "    # print(dB_x_delta, logPr)\n",
    "    # loss = dB_x_delta - ctc_constant * logPr\n",
    "    loss = - logPr\n",
    "\n",
    "    # check if dbloss is smaller than eps\n",
    "    if dB_x_delta < eps:\n",
    "        return loss\n",
    "    else:\n",
    "        # print(dB_x_delta, eps)\n",
    "        # return None\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss\n",
      "tensor(5.8390, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "final loss\n",
      "tensor(5.9684, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctc_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctc_constant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# break if loss is None\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[25], line 38\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(audio, noise, target_logits, model, eps, ctc_constant)\u001b[0m\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(target_logits.shape, target_logits.dtype)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m logPr \u001b[38;5;241m=\u001b[39m \u001b[43mctcloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"dB_x_delta, logPr\")\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# print(dB_x_delta, logPr)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# loss = dB_x_delta - ctc_constant * logPr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m logPr\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1770\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\functional.py:2656\u001b[0m, in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[0;32m   2650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2651\u001b[0m         ctc_loss,\n\u001b[0;32m   2652\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   2653\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[0;32m   2654\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[0;32m   2655\u001b[0m     )\n\u001b[1;32m-> 2656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define eps\n",
    "eps = 10\n",
    "# define number of iterations\n",
    "n = 5000\n",
    "# define learning rate\n",
    "lr = 1e-1\n",
    "# define perturbed audio: start with clean audio\n",
    "noise = torch.zeros_like(audio).requires_grad_(True)\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "ctc_constant = 1\n",
    "\n",
    "# loop over n iterations\n",
    "for i in range(n):\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # calculate loss\n",
    "    loss = loss_function(audio, noise, target_logits, model, eps, ctc_constant=ctc_constant)\n",
    "    # break if loss is None\n",
    "    if loss is None:\n",
    "        break\n",
    "    print(\"final loss\")\n",
    "    print(loss)      \n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update perturbation\n",
    "    optimizer.step()\n",
    "#    print(audio_pert)\n",
    "# save adversarial example\n",
    "audio_pert = (audio+noise).detach().to(\"cpu\")\n",
    "torchaudio.save(\"adversarial_one.wav\", audio_pert, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.0689, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3960265 ,  0.37243792, -0.37965736, ...,  0.33345932,\n",
       "        -0.42485094,  0.32958943]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3785, -0.3786, -0.3785,  ..., -0.4242, -0.4231, -0.4313]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display audio object\n",
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.0002,  0.0002,  ..., -0.0454, -0.0443, -0.0526]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxed = torch.nn.Softmax(dim=1)\n",
    "probs = softmaxed(model(perturbed_audio).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"THE WILL BURN YOU TO A CRISP\"\n",
    "#convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_audio = audio + audio_pert\n",
    "dB_x = 20 * torch.log10(torch.norm(audio) / torch.norm(audio_pert))\n",
    "# calculate logPr(t|f(x+delta))\n",
    "logits = model(perturbed_audio).logits\n",
    "pred = processor.batch_decode(torch.argmax(logits, dim=-1))\n",
    "\n",
    "#print(target)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
