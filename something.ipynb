{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# libary to read audio in torch\n",
    "import torchaudio\n",
    "from torch.nn import CTCLoss\n",
    "from jiwer import wer\n",
    "import soundfile as sf\n",
    "\n",
    "softmax = torch.nn.LogSoftmax(dim=1)\n",
    "ctcloss = CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #size for wav2vec2-large-960h\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# print(\"Size of wav2vec2-large-960h: \", model.num_parameters())\n",
    "# print(model)\n",
    "# #size for wav2vec2-base-960h\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# print(\"Size of wav2vec2-base-960h: \", model.num_parameters())\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# example of tokenizing an audio file\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select an audio file and process it\n",
    "#question: is it possible to process the audio after adding noise to it and backpropagate the loss?\n",
    "audio = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values\n",
    "sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "#sentence we want our model to predict\n",
    "target = \"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"\n",
    "target = target.replace(\" \", \"|\")\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target]\n",
    "# convert to tensor logits using the tokenizer\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#load everything to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss(logits, targets):\n",
    "    #this function only tested for batch_size = 1\n",
    "    input_lengths = torch.tensor([logits.shape[0]])\n",
    "    target_lengths = torch.tensor([targets.shape[0]])\n",
    "    return ctcloss(logits, targets, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(audio, noise, target_logits, model, reg_weight, ctc_weight, eps):\n",
    "    \"\"\" \n",
    "    Computes the loss of the audio with the current noise added, with a factor to control the size of the noise (via regularization) to allow for backpropagation of the input audio signal.\n",
    "    audio: original audio after processing (for now)\n",
    "    noise: noise to be added to the audio\n",
    "    target_logits: target logits for the sentence we want to generate an attack for\n",
    "    model: model to be attacked\n",
    "    reg_weight: weight for the noise regularization term\n",
    "    ctc_weight: weight for the ctc loss\n",
    "    eps: maximum perturbation allowed\n",
    "    \"\"\"\n",
    "    audio_perturbed = audio + noise\n",
    "    #compute dB_x\n",
    "    dB_x = (20 * torch.log10(audio-audio.min())).max()\n",
    "    #compute dB_delta\n",
    "    dB_delta = (20 * torch.log10(noise-noise.min())).max()\n",
    "    #compute dB_x_delta\n",
    "    dB_x_delta = dB_delta - dB_x\n",
    "    #compute logits\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = softmax(logits[0])\n",
    "    logits = logits.unsqueeze(1)\n",
    "    #compute ctc loss\n",
    "    ctc_loss_value = ctc_loss(logits, target_logits)\n",
    "    #compute noise regularization\n",
    "    noise_reg = torch.norm(noise, p=2)\n",
    "    #compute total loss\n",
    "    loss = reg_weight * noise_reg + ctc_weight * ctc_loss_value\n",
    "    if dB_x_delta < eps:\n",
    "        return loss, ctc_loss_value.item(), noise_reg.item(), dB_x_delta.item()\n",
    "    else:\n",
    "        print(loss.item(), ctc_loss_value.item(), noise_reg.item(), dB_x_delta.item())\n",
    "        return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88.79, 17.76, 0.0, -inf]]\n",
      "[[80.85, 16.11, 0.31, -75.37], [73.6, 14.62, 0.51, -69.34], [70.87, 14.04, 0.68, -65.81], [66.76, 13.18, 0.85, -63.31], [62.82, 12.36, 1.01, -61.36], [59.3, 11.63, 1.17, -59.76], [55.79, 10.9, 1.31, -58.41], [53.2, 10.35, 1.45, -57.23], [50.23, 9.73, 1.58, -56.19], [47.74, 9.21, 1.71, -55.28]]\n",
      "[[45.74, 8.78, 1.83, -54.44], [44.01, 8.41, 1.95, -53.68], [42.23, 8.03, 2.06, -52.97], [40.73, 7.71, 2.17, -52.3], [39.54, 7.45, 2.28, -51.67], [38.4, 7.2, 2.38, -51.08], [37.27, 6.96, 2.48, -50.52], [36.16, 6.72, 2.57, -49.99], [35.13, 6.49, 2.66, -49.49], [34.1, 6.27, 2.75, -49.02]]\n",
      "[[33.11, 6.06, 2.84, -48.57], [32.3, 5.88, 2.92, -48.15], [31.51, 5.7, 2.99, -47.81], [30.82, 5.55, 3.07, -47.48], [30.06, 5.39, 3.14, -47.11], [29.42, 5.24, 3.2, -46.75], [28.95, 5.14, 3.27, -46.41], [28.62, 5.06, 3.33, -46.09], [28.1, 4.94, 3.39, -45.79], [27.31, 4.77, 3.45, -45.49]]\n",
      "[[26.73, 4.65, 3.5, -45.21], [26.33, 4.56, 3.56, -44.95], [25.84, 4.45, 3.6, -44.72], [25.25, 4.32, 3.65, -44.5], [24.73, 4.21, 3.69, -44.3], [24.22, 4.1, 3.74, -44.11], [23.83, 4.01, 3.78, -43.94], [23.4, 3.92, 3.81, -43.77], [22.97, 3.83, 3.85, -43.57], [22.66, 3.76, 3.88, -43.35]]\n",
      "[[22.39, 3.7, 3.91, -43.14], [22.13, 3.64, 3.94, -42.98], [21.84, 3.57, 3.97, -42.84], [21.54, 3.51, 3.99, -42.71], [21.32, 3.46, 4.01, -42.59], [21.19, 3.43, 4.03, -42.46], [20.92, 3.38, 4.05, -42.3], [20.72, 3.33, 4.06, -42.15], [20.54, 3.29, 4.07, -42.01], [20.38, 3.26, 4.08, -41.89]]\n",
      "[[20.2, 3.22, 4.1, -41.77], [20.03, 3.19, 4.11, -41.61], [19.86, 3.15, 4.11, -41.46], [19.7, 3.12, 4.12, -41.34], [19.56, 3.09, 4.13, -41.23], [19.44, 3.06, 4.13, -41.13], [19.28, 3.03, 4.14, -41.06], [19.14, 3.0, 4.14, -41.0], [19.02, 2.98, 4.14, -40.95], [18.91, 2.95, 4.14, -40.9]]\n",
      "[[18.81, 2.93, 4.14, -40.86], [18.7, 2.91, 4.14, -40.8], [18.57, 2.89, 4.13, -40.73], [18.46, 2.87, 4.13, -40.66], [18.36, 2.85, 4.13, -40.59], [18.26, 2.83, 4.12, -40.52], [18.15, 2.81, 4.12, -40.45], [18.05, 2.79, 4.11, -40.38], [17.95, 2.77, 4.11, -40.31], [17.85, 2.75, 4.1, -40.22]]\n",
      "[[17.77, 2.73, 4.1, -40.14], [17.66, 2.71, 4.09, -40.06], [17.57, 2.7, 4.08, -40.0], [17.49, 2.68, 4.08, -39.95], [17.4, 2.67, 4.07, -39.9], [17.32, 2.65, 4.06, -39.85], [17.25, 2.64, 4.05, -39.82], [17.17, 2.63, 4.04, -39.78], [17.11, 2.62, 4.03, -39.75], [17.03, 2.6, 4.02, -39.73]]\n",
      "[[16.96, 2.59, 4.01, -39.71], [16.89, 2.58, 4.0, -39.69], [16.81, 2.56, 3.99, -39.68], [16.75, 2.55, 3.98, -39.68], [16.8, 2.57, 3.97, -39.65], [16.89, 2.59, 3.95, -39.63], [16.85, 2.58, 3.94, -39.61], [16.76, 2.57, 3.93, -39.6], [16.7, 2.56, 3.92, -39.59], [16.65, 2.55, 3.91, -39.58]]\n",
      "[[16.6, 2.54, 3.9, -39.57], [16.55, 2.53, 3.89, -39.57], [16.5, 2.52, 3.88, -39.57], [16.44, 2.51, 3.87, -39.57], [16.39, 2.5, 3.87, -39.57], [16.32, 2.49, 3.86, -39.57], [16.25, 2.48, 3.85, -39.58], [16.17, 2.47, 3.85, -39.58], [16.1, 2.45, 3.84, -39.58], [16.02, 2.44, 3.83, -39.59]]\n",
      "[[15.95, 2.42, 3.83, -39.59], [15.88, 2.41, 3.83, -39.59], [15.83, 2.4, 3.82, -39.55], [15.79, 2.4, 3.82, -39.5], [15.75, 2.39, 3.81, -39.45], [15.67, 2.37, 3.81, -39.4], [15.6, 2.36, 3.81, -39.35], [15.65, 2.37, 3.8, -39.3], [15.53, 2.35, 3.8, -39.25], [15.42, 2.32, 3.8, -39.18]]\n",
      "[[15.36, 2.31, 3.79, -39.1], [15.31, 2.3, 3.79, -39.04], [15.25, 2.29, 3.79, -38.98], [15.16, 2.27, 3.79, -38.88], [15.04, 2.25, 3.79, -38.79], [14.92, 2.23, 3.79, -38.69], [14.82, 2.21, 3.8, -38.6], [14.75, 2.19, 3.8, -38.52], [14.67, 2.17, 3.8, -38.44], [14.63, 2.17, 3.81, -38.36]]\n",
      "[[14.56, 2.15, 3.81, -38.29], [14.53, 2.14, 3.81, -38.19], [14.49, 2.14, 3.8, -38.11], [14.45, 2.13, 3.8, -38.05], [14.41, 2.12, 3.79, -37.99], [14.37, 2.11, 3.79, -37.95], [14.34, 2.11, 3.79, -37.91], [14.29, 2.1, 3.78, -37.89], [14.24, 2.09, 3.78, -37.87], [14.2, 2.09, 3.77, -37.87]]\n",
      "[[14.17, 2.08, 3.76, -37.87], [14.14, 2.08, 3.75, -37.87], [14.1, 2.07, 3.74, -37.89], [14.05, 2.06, 3.74, -37.9], [14.02, 2.06, 3.73, -37.92], [13.99, 2.05, 3.72, -37.94], [13.95, 2.05, 3.71, -37.97], [13.9, 2.04, 3.7, -37.99], [13.86, 2.03, 3.7, -38.02], [13.82, 2.03, 3.69, -38.04]]\n",
      "[[13.79, 2.02, 3.68, -38.07], [13.74, 2.01, 3.67, -38.1], [13.71, 2.01, 3.67, -38.12], [13.68, 2.0, 3.66, -38.15], [13.64, 2.0, 3.65, -38.16], [13.61, 1.99, 3.65, -38.18], [13.57, 1.99, 3.64, -38.19], [13.53, 1.98, 3.63, -38.19], [13.51, 1.98, 3.63, -38.19], [13.47, 1.97, 3.62, -38.19]]\n",
      "[[13.44, 1.96, 3.62, -38.19], [13.41, 1.96, 3.61, -38.19], [13.38, 1.95, 3.61, -38.19], [13.35, 1.95, 3.6, -38.18], [13.32, 1.94, 3.6, -38.15], [13.29, 1.94, 3.59, -38.12], [13.26, 1.93, 3.59, -38.09], [13.23, 1.93, 3.58, -38.06], [13.2, 1.93, 3.57, -38.04], [13.19, 1.92, 3.57, -38.01]]\n",
      "[[13.17, 1.92, 3.56, -38.01], [13.15, 1.92, 3.56, -37.99], [13.12, 1.91, 3.55, -37.98], [13.09, 1.91, 3.55, -37.95], [13.07, 1.91, 3.54, -37.93], [13.04, 1.9, 3.54, -37.9], [13.01, 1.9, 3.53, -37.86], [12.99, 1.89, 3.53, -37.83], [12.97, 1.89, 3.53, -37.8], [12.93, 1.88, 3.52, -37.76]]\n",
      "[[12.91, 1.88, 3.52, -37.73], [12.9, 1.88, 3.51, -37.69], [12.89, 1.88, 3.51, -37.64], [12.86, 1.87, 3.51, -37.59], [12.84, 1.87, 3.51, -37.55], [12.81, 1.86, 3.5, -37.51], [12.78, 1.86, 3.5, -37.47], [12.76, 1.85, 3.5, -37.44], [12.73, 1.85, 3.49, -37.36], [12.7, 1.84, 3.49, -37.28]]\n",
      "[[12.68, 1.84, 3.49, -37.21], [12.65, 1.83, 3.48, -37.14], [12.64, 1.83, 3.48, -37.07], [12.62, 1.83, 3.48, -37.0], [12.61, 1.83, 3.47, -36.94], [12.6, 1.83, 3.47, -36.86], [12.59, 1.82, 3.47, -36.79], [12.58, 1.82, 3.47, -36.73], [12.55, 1.82, 3.46, -36.66], [12.53, 1.81, 3.46, -36.59]]\n",
      "[[12.51, 1.81, 3.46, -36.52], [12.48, 1.8, 3.46, -36.45], [12.45, 1.8, 3.46, -36.39], [12.43, 1.79, 3.46, -36.33], [12.4, 1.79, 3.46, -36.26], [12.37, 1.78, 3.46, -36.18], [12.35, 1.78, 3.45, -36.12], [12.34, 1.78, 3.45, -36.05], [12.31, 1.77, 3.45, -35.98], [12.28, 1.77, 3.45, -35.9]]\n",
      "[[12.27, 1.76, 3.45, -35.83], [12.24, 1.76, 3.45, -35.77], [12.22, 1.75, 3.45, -35.73], [12.2, 1.75, 3.45, -35.7], [12.17, 1.75, 3.45, -35.67], [12.15, 1.74, 3.45, -35.65], [12.12, 1.74, 3.44, -35.64], [12.1, 1.73, 3.44, -35.64], [12.08, 1.73, 3.44, -35.64], [12.06, 1.72, 3.44, -35.64]]\n",
      "[[12.04, 1.72, 3.44, -35.64], [12.02, 1.72, 3.44, -35.65], [12.0, 1.71, 3.43, -35.66], [11.98, 1.71, 3.43, -35.66], [11.96, 1.71, 3.43, -35.64], [11.94, 1.7, 3.43, -35.63], [11.92, 1.7, 3.42, -35.61], [11.9, 1.7, 3.42, -35.6], [11.88, 1.69, 3.42, -35.58], [11.87, 1.69, 3.42, -35.56]]\n",
      "[[11.85, 1.69, 3.42, -35.55], [11.83, 1.68, 3.42, -35.53], [11.81, 1.68, 3.41, -35.51], [11.79, 1.68, 3.41, -35.48], [11.78, 1.67, 3.41, -35.46], [11.76, 1.67, 3.41, -35.44], [11.74, 1.67, 3.41, -35.42], [11.72, 1.66, 3.41, -35.4], [11.71, 1.66, 3.41, -35.38], [11.69, 1.66, 3.4, -35.32]]\n",
      "[[11.67, 1.65, 3.4, -35.25], [11.65, 1.65, 3.4, -35.19], [11.64, 1.65, 3.4, -35.13], [11.62, 1.64, 3.4, -35.08], [11.6, 1.64, 3.4, -35.03], [11.58, 1.64, 3.4, -34.98], [11.57, 1.63, 3.4, -34.94], [11.55, 1.63, 3.4, -34.9], [11.53, 1.63, 3.4, -34.86], [11.51, 1.62, 3.4, -34.83]]\n",
      "[[11.49, 1.62, 3.4, -34.8], [11.47, 1.61, 3.4, -34.77], [11.45, 1.61, 3.4, -34.74], [11.43, 1.61, 3.4, -34.71], [11.41, 1.6, 3.4, -34.69], [11.38, 1.6, 3.4, -34.66], [11.36, 1.59, 3.41, -34.63], [11.33, 1.58, 3.41, -34.58], [11.3, 1.58, 3.41, -34.53], [11.27, 1.57, 3.41, -34.47]]\n",
      "[[11.24, 1.56, 3.42, -34.41], [11.23, 1.56, 3.42, -34.36], [11.22, 1.56, 3.43, -34.31], [11.28, 1.57, 3.43, -34.28], [11.19, 1.55, 3.43, -34.24], [11.3, 1.57, 3.44, -34.18], [11.42, 1.6, 3.44, -34.19], [11.43, 1.6, 3.45, -34.15], [11.39, 1.59, 3.45, -34.11], [11.3, 1.57, 3.46, -34.08]]\n",
      "[[11.39, 1.58, 3.46, -34.06], [11.23, 1.55, 3.46, -34.05], [11.37, 1.58, 3.46, -34.04], [11.26, 1.56, 3.46, -34.03], [11.11, 1.53, 3.46, -34.02], [11.2, 1.55, 3.46, -34.02], [11.12, 1.53, 3.46, -34.02], [11.12, 1.53, 3.47, -34.01], [11.11, 1.53, 3.47, -34.01], [11.0, 1.51, 3.47, -34.0]]\n",
      "[[11.07, 1.52, 3.47, -34.0], [10.97, 1.5, 3.47, -34.0], [10.96, 1.5, 3.47, -34.0], [10.96, 1.5, 3.48, -33.99], [10.93, 1.49, 3.48, -33.99], [10.94, 1.49, 3.48, -33.99], [10.88, 1.48, 3.48, -33.98], [10.83, 1.47, 3.48, -33.97], [10.87, 1.48, 3.47, -33.96], [10.81, 1.47, 3.47, -33.97]]\n",
      "[[10.79, 1.46, 3.47, -33.98], [10.78, 1.46, 3.47, -33.97], [10.76, 1.46, 3.47, -33.97], [10.75, 1.46, 3.47, -33.96], [10.72, 1.45, 3.47, -33.95], [10.7, 1.45, 3.47, -33.95], [10.69, 1.44, 3.47, -33.94], [10.67, 1.44, 3.47, -33.93], [10.65, 1.44, 3.47, -33.93], [10.63, 1.43, 3.47, -33.93]]\n",
      "[[10.62, 1.43, 3.47, -33.93], [10.61, 1.43, 3.47, -33.93], [10.6, 1.43, 3.47, -33.93], [10.57, 1.42, 3.47, -33.93], [10.56, 1.42, 3.47, -33.93], [10.67, 1.44, 3.46, -33.93], [10.66, 1.44, 3.46, -33.93], [10.67, 1.44, 3.46, -33.92], [10.69, 1.45, 3.46, -33.92], [10.6, 1.43, 3.46, -33.9]]\n",
      "[[10.58, 1.42, 3.47, -33.88], [10.59, 1.42, 3.47, -33.87], [10.54, 1.41, 3.47, -33.85], [10.52, 1.41, 3.47, -33.84], [11.13, 1.53, 3.48, -33.81], [10.73, 1.45, 3.47, -33.85], [10.93, 1.49, 3.47, -33.88], [10.89, 1.49, 3.47, -33.91], [10.88, 1.48, 3.47, -33.93], [10.93, 1.49, 3.47, -33.95]]\n",
      "[[10.78, 1.46, 3.48, -33.96], [10.82, 1.47, 3.49, -33.96], [10.75, 1.45, 3.5, -33.95], [10.66, 1.43, 3.5, -33.92], [10.65, 1.43, 3.51, -33.9], [10.61, 1.42, 3.52, -33.88], [10.57, 1.41, 3.53, -33.86], [10.54, 1.4, 3.53, -33.84], [10.53, 1.4, 3.54, -33.83], [10.63, 1.42, 3.55, -33.81]]\n",
      "[[10.61, 1.41, 3.56, -33.8], [10.68, 1.42, 3.57, -33.79], [10.87, 1.46, 3.57, -33.79], [10.8, 1.45, 3.57, -33.8], [10.73, 1.43, 3.57, -33.82], [10.65, 1.42, 3.57, -33.84], [10.63, 1.41, 3.56, -33.85], [10.61, 1.41, 3.56, -33.86], [10.9, 1.47, 3.56, -33.85], [10.87, 1.46, 3.57, -33.84]]\n",
      "[[10.67, 1.42, 3.57, -33.84], [10.7, 1.43, 3.57, -33.84], [10.66, 1.42, 3.58, -33.84], [10.62, 1.41, 3.58, -33.84], [10.56, 1.4, 3.59, -33.84], [10.73, 1.43, 3.59, -33.83], [11.07, 1.49, 3.6, -33.83], [10.84, 1.45, 3.61, -33.8], [10.77, 1.43, 3.62, -33.79], [10.71, 1.41, 3.63, -33.77]]\n",
      "[[10.77, 1.42, 3.65, -33.76], [10.66, 1.4, 3.66, -33.74], [10.6, 1.38, 3.68, -33.72], [10.62, 1.39, 3.69, -33.7], [10.6, 1.38, 3.7, -33.7], [10.56, 1.37, 3.71, -33.69], [10.56, 1.37, 3.71, -33.68], [10.49, 1.35, 3.72, -33.67], [10.46, 1.35, 3.72, -33.66], [10.4, 1.33, 3.73, -33.64]]\n",
      "[[10.38, 1.33, 3.73, -33.63], [10.34, 1.32, 3.73, -33.62], [10.31, 1.31, 3.74, -33.61], [10.26, 1.3, 3.74, -33.6], [10.22, 1.29, 3.75, -33.59], [10.18, 1.29, 3.75, -33.58], [10.16, 1.28, 3.75, -33.57], [10.12, 1.27, 3.75, -33.57], [10.09, 1.27, 3.75, -33.57], [10.06, 1.26, 3.75, -33.57]]\n",
      "[[10.03, 1.26, 3.74, -33.57], [9.99, 1.25, 3.74, -33.58], [9.97, 1.25, 3.74, -33.58], [9.93, 1.24, 3.74, -33.59], [9.91, 1.23, 3.73, -33.6], [9.88, 1.23, 3.73, -33.61], [9.85, 1.22, 3.73, -33.62], [9.83, 1.22, 3.73, -33.63], [9.8, 1.22, 3.72, -33.64], [9.77, 1.21, 3.72, -33.64]]\n",
      "[[9.75, 1.21, 3.72, -33.65], [9.72, 1.2, 3.71, -33.66], [9.7, 1.2, 3.71, -33.66], [9.68, 1.19, 3.71, -33.67], [9.69, 1.2, 3.71, -33.68], [9.71, 1.2, 3.71, -33.68], [9.64, 1.19, 3.71, -33.68], [9.64, 1.19, 3.71, -33.68], [9.65, 1.19, 3.71, -33.68], [9.57, 1.17, 3.71, -33.68]]\n",
      "[[9.62, 1.18, 3.71, -33.67], [9.65, 1.19, 3.71, -33.68], [9.56, 1.17, 3.71, -33.68], [9.66, 1.19, 3.71, -33.68], [9.61, 1.18, 3.71, -33.68], [9.59, 1.18, 3.71, -33.68], [9.54, 1.17, 3.71, -33.68], [9.51, 1.16, 3.71, -33.68], [9.51, 1.16, 3.71, -33.68], [9.46, 1.15, 3.71, -33.69]]\n",
      "[[9.47, 1.15, 3.71, -33.69], [9.4, 1.14, 3.71, -33.7], [9.4, 1.14, 3.71, -33.7], [9.35, 1.13, 3.72, -33.69], [9.36, 1.13, 3.72, -33.7], [9.3, 1.12, 3.72, -33.7], [9.3, 1.12, 3.72, -33.7], [9.27, 1.11, 3.72, -33.7], [9.25, 1.11, 3.72, -33.71], [9.24, 1.1, 3.72, -33.71]]\n",
      "[[9.21, 1.1, 3.72, -33.72], [9.21, 1.1, 3.72, -33.73], [9.18, 1.09, 3.72, -33.74], [9.16, 1.09, 3.72, -33.74], [9.15, 1.09, 3.72, -33.74], [9.12, 1.08, 3.72, -33.75], [9.1, 1.08, 3.72, -33.75], [9.1, 1.08, 3.72, -33.75], [9.09, 1.07, 3.72, -33.75], [9.06, 1.07, 3.72, -33.75]]\n",
      "[[9.07, 1.07, 3.72, -33.75], [9.06, 1.07, 3.72, -33.76], [9.04, 1.06, 3.72, -33.76], [9.02, 1.06, 3.72, -33.77], [9.0, 1.06, 3.72, -33.78], [8.99, 1.05, 3.72, -33.79], [8.98, 1.05, 3.72, -33.8], [8.96, 1.05, 3.72, -33.8], [8.95, 1.05, 3.72, -33.8], [8.94, 1.04, 3.72, -33.8]]\n",
      "[[8.93, 1.04, 3.72, -33.8], [8.93, 1.04, 3.72, -33.8], [8.91, 1.04, 3.72, -33.8], [8.89, 1.03, 3.72, -33.79], [8.86, 1.03, 3.72, -33.79], [8.85, 1.03, 3.72, -33.8], [8.83, 1.02, 3.72, -33.8], [8.82, 1.02, 3.72, -33.8], [8.84, 1.03, 3.72, -33.79], [8.84, 1.02, 3.72, -33.8]]\n",
      "[[8.93, 1.04, 3.72, -33.79], [8.81, 1.02, 3.72, -33.79], [8.8, 1.02, 3.72, -33.79], [9.03, 1.06, 3.72, -33.79], [8.96, 1.05, 3.72, -33.8], [9.0, 1.05, 3.73, -33.82], [9.04, 1.06, 3.73, -33.83], [8.97, 1.05, 3.73, -33.85], [8.93, 1.04, 3.73, -33.87], [8.93, 1.04, 3.73, -33.88]]\n",
      "[[8.84, 1.02, 3.73, -33.88], [8.91, 1.03, 3.73, -33.87], [8.82, 1.02, 3.74, -33.86], [8.83, 1.02, 3.74, -33.84], [8.78, 1.01, 3.74, -33.83], [8.76, 1.0, 3.74, -33.82], [8.76, 1.0, 3.75, -33.81], [8.72, 1.0, 3.75, -33.81], [8.72, 1.0, 3.74, -33.81], [8.69, 0.99, 3.75, -33.81]]\n",
      "[[8.67, 0.99, 3.75, -33.8], [8.65, 0.98, 3.75, -33.81], [8.68, 0.99, 3.75, -33.81], [8.65, 0.98, 3.75, -33.82], [8.65, 0.98, 3.75, -33.83], [8.61, 0.97, 3.75, -33.83], [8.63, 0.98, 3.75, -33.83], [8.61, 0.97, 3.75, -33.83], [8.61, 0.97, 3.75, -33.84], [8.63, 0.98, 3.75, -33.84]]\n",
      "[[8.95, 1.04, 3.75, -33.83], [8.73, 0.99, 3.75, -33.83], [8.96, 1.04, 3.76, -33.84], [8.87, 1.02, 3.76, -33.84], [8.85, 1.02, 3.76, -33.84], [8.69, 0.98, 3.77, -33.84], [8.83, 1.01, 3.78, -33.83], [8.67, 0.98, 3.79, -33.83], [8.85, 1.01, 3.79, -33.84], [8.92, 1.03, 3.79, -33.85]]\n",
      "[[9.59, 1.16, 3.79, -33.84], [8.85, 1.01, 3.8, -33.84], [9.62, 1.16, 3.81, -33.85], [9.16, 1.07, 3.82, -33.85], [9.87, 1.21, 3.84, -33.86], [9.57, 1.14, 3.85, -33.88], [9.54, 1.13, 3.87, -33.89], [9.48, 1.12, 3.89, -33.89], [9.23, 1.06, 3.91, -33.84], [9.22, 1.06, 3.93, -33.77]]\n",
      "[[9.28, 1.07, 3.95, -33.71], [9.26, 1.06, 3.97, -33.66], [9.2, 1.04, 3.98, -33.61], [9.21, 1.04, 3.99, -33.57], [9.43, 1.09, 4.0, -33.53], [9.56, 1.11, 4.01, -33.5], [9.45, 1.09, 4.02, -33.48], [9.38, 1.07, 4.02, -33.48], [9.28, 1.05, 4.03, -33.48], [9.16, 1.03, 4.03, -33.48]]\n",
      "[[9.15, 1.02, 4.03, -33.49], [9.16, 1.03, 4.03, -33.5], [9.09, 1.01, 4.03, -33.5], [9.05, 1.0, 4.04, -33.5], [9.09, 1.01, 4.04, -33.48], [8.93, 0.98, 4.04, -33.48], [8.97, 0.99, 4.04, -33.46], [8.87, 0.97, 4.04, -33.44], [8.81, 0.95, 4.04, -33.41], [8.8, 0.95, 4.03, -33.38]]\n",
      "[[8.76, 0.94, 4.03, -33.36], [8.72, 0.94, 4.03, -33.34], [8.71, 0.94, 4.02, -33.33], [8.67, 0.93, 4.02, -33.32], [8.62, 0.92, 4.02, -33.31], [8.59, 0.92, 4.01, -33.31], [8.57, 0.91, 4.01, -33.3], [8.54, 0.91, 4.0, -33.3], [8.52, 0.9, 4.0, -33.29], [8.51, 0.9, 3.99, -33.29]]\n",
      "[[8.46, 0.9, 3.99, -33.28], [8.45, 0.89, 3.98, -33.28], [8.45, 0.89, 3.98, -33.27], [8.38, 0.88, 3.97, -33.27], [8.36, 0.88, 3.97, -33.26], [8.39, 0.88, 3.97, -33.26], [8.36, 0.88, 3.96, -33.26], [8.34, 0.88, 3.96, -33.25], [8.27, 0.86, 3.96, -33.25], [8.27, 0.86, 3.95, -33.25]]\n",
      "[[8.29, 0.87, 3.95, -33.25], [8.26, 0.86, 3.95, -33.25], [8.27, 0.86, 3.94, -33.24], [8.33, 0.88, 3.94, -33.24], [8.23, 0.86, 3.94, -33.24], [8.25, 0.86, 3.93, -33.23], [8.21, 0.86, 3.93, -33.23], [8.17, 0.85, 3.93, -33.23], [8.21, 0.86, 3.93, -33.23], [8.22, 0.86, 3.93, -33.22]]\n",
      "[[8.23, 0.86, 3.93, -33.22], [8.23, 0.86, 3.93, -33.2], [8.24, 0.86, 3.93, -33.2], [8.25, 0.87, 3.93, -33.19], [8.46, 0.91, 3.93, -33.19], [8.54, 0.92, 3.93, -33.17], [8.75, 0.97, 3.93, -33.17], [8.41, 0.89, 3.93, -33.17], [8.71, 0.95, 3.94, -33.17], [8.63, 0.94, 3.94, -33.17]]\n",
      "[[8.75, 0.96, 3.95, -33.16], [8.65, 0.94, 3.96, -33.15], [8.66, 0.94, 3.97, -33.12], [8.55, 0.92, 3.97, -33.1], [8.55, 0.92, 3.98, -33.08], [8.48, 0.9, 3.98, -33.07], [8.46, 0.9, 3.98, -33.07], [8.46, 0.89, 3.99, -33.06], [8.37, 0.87, 4.0, -33.06], [8.31, 0.86, 4.01, -33.05]]\n",
      "[[8.31, 0.86, 4.01, -33.04], [8.27, 0.85, 4.01, -33.02], [8.27, 0.85, 4.01, -33.0], [8.22, 0.84, 4.01, -32.98], [8.18, 0.83, 4.01, -32.97], [8.2, 0.84, 4.01, -32.95], [8.18, 0.83, 4.01, -32.93], [8.14, 0.83, 4.01, -32.92], [8.15, 0.83, 4.0, -32.91], [8.11, 0.82, 4.0, -32.89]]\n",
      "[[8.05, 0.81, 3.99, -32.88], [8.07, 0.82, 3.99, -32.87], [8.08, 0.82, 3.99, -32.85], [7.99, 0.8, 3.99, -32.83], [8.01, 0.81, 3.98, -32.82], [8.08, 0.82, 3.98, -32.8], [7.98, 0.8, 3.98, -32.78], [8.03, 0.81, 3.98, -32.77], [8.09, 0.82, 3.98, -32.75], [8.0, 0.8, 3.98, -32.74]]\n",
      "[[8.05, 0.81, 3.97, -32.73], [7.93, 0.79, 3.98, -32.71], [7.99, 0.8, 3.98, -32.69], [7.9, 0.79, 3.97, -32.68], [7.93, 0.79, 3.97, -32.67], [7.92, 0.79, 3.97, -32.65], [7.86, 0.78, 3.97, -32.63], [7.97, 0.8, 3.97, -32.61], [8.17, 0.84, 3.97, -32.6], [8.26, 0.86, 3.97, -32.59]]\n",
      "[[8.21, 0.85, 3.97, -32.58], [8.29, 0.86, 3.97, -32.57], [8.09, 0.82, 3.98, -32.56], [8.05, 0.81, 3.98, -32.54], [8.1, 0.82, 3.99, -32.53], [7.94, 0.79, 3.99, -32.52], [8.09, 0.82, 3.99, -32.51], [8.03, 0.81, 3.99, -32.5], [7.9, 0.78, 3.99, -32.5], [7.9, 0.78, 3.99, -32.49]]\n",
      "[[7.89, 0.78, 3.99, -32.48], [7.88, 0.78, 3.99, -32.47], [7.8, 0.76, 3.98, -32.46], [7.85, 0.77, 3.98, -32.45], [7.85, 0.77, 3.99, -32.43], [7.84, 0.77, 3.99, -32.42], [7.77, 0.76, 3.99, -32.41], [7.74, 0.75, 3.99, -32.4], [7.91, 0.79, 3.99, -32.4], [8.04, 0.81, 3.99, -32.41]]\n",
      "[[8.24, 0.85, 3.99, -32.42], [8.39, 0.88, 3.99, -32.42], [8.35, 0.87, 4.0, -32.42], [8.05, 0.81, 4.01, -32.41], [8.32, 0.86, 4.01, -32.39], [8.69, 0.94, 4.02, -32.36], [9.14, 1.02, 4.02, -32.35], [8.75, 0.94, 4.03, -32.35], [8.68, 0.93, 4.04, -32.36], [8.67, 0.92, 4.06, -32.37]]\n",
      "[[8.59, 0.91, 4.07, -32.37], [8.33, 0.85, 4.08, -32.37], [8.52, 0.89, 4.08, -32.37], [8.96, 0.97, 4.09, -32.37], [8.77, 0.93, 4.11, -32.37], [8.46, 0.87, 4.12, -32.38], [8.65, 0.9, 4.14, -32.37], [8.62, 0.89, 4.15, -32.36], [8.49, 0.86, 4.17, -32.34], [8.28, 0.82, 4.19, -32.31]]\n",
      "[[8.25, 0.81, 4.2, -32.29], [8.25, 0.81, 4.21, -32.28], [8.18, 0.79, 4.22, -32.27], [8.13, 0.78, 4.22, -32.27], [8.1, 0.78, 4.22, -32.28], [8.11, 0.78, 4.22, -32.29], [8.02, 0.76, 4.22, -32.3], [8.05, 0.77, 4.23, -32.31], [7.89, 0.73, 4.23, -32.32], [7.9, 0.73, 4.23, -32.33]]\n",
      "[[7.85, 0.72, 4.23, -32.35], [7.82, 0.72, 4.23, -32.36], [7.78, 0.71, 4.22, -32.38], [7.78, 0.71, 4.22, -32.41], [7.78, 0.71, 4.21, -32.43], [7.74, 0.71, 4.2, -32.45], [7.73, 0.71, 4.2, -32.47], [7.69, 0.7, 4.19, -32.48], [7.64, 0.69, 4.18, -32.49], [7.6, 0.69, 4.17, -32.49]]\n",
      "[[7.59, 0.68, 4.17, -32.5], [7.6, 0.69, 4.16, -32.52], [7.58, 0.68, 4.16, -32.53], [7.56, 0.68, 4.16, -32.54], [7.56, 0.68, 4.15, -32.54], [7.56, 0.68, 4.15, -32.54], [7.51, 0.67, 4.15, -32.54], [7.48, 0.67, 4.15, -32.54], [7.48, 0.67, 4.15, -32.54], [7.42, 0.66, 4.14, -32.54]]\n",
      "[[7.43, 0.66, 4.14, -32.54], [7.41, 0.66, 4.14, -32.54], [7.34, 0.64, 4.13, -32.55], [7.36, 0.65, 4.13, -32.56], [7.39, 0.65, 4.13, -32.56], [7.35, 0.64, 4.13, -32.56], [7.33, 0.64, 4.13, -32.57], [7.46, 0.67, 4.12, -32.58], [7.46, 0.67, 4.12, -32.59], [7.55, 0.68, 4.12, -32.6]]\n",
      "[[7.63, 0.7, 4.12, -32.61], [7.42, 0.66, 4.12, -32.64], [7.32, 0.64, 4.12, -32.66], [7.43, 0.66, 4.12, -32.66], [7.31, 0.64, 4.12, -32.66], [7.29, 0.63, 4.13, -32.64], [9.23, 1.02, 4.13, -32.65], [8.88, 0.95, 4.14, -32.67], [8.49, 0.87, 4.16, -32.69], [8.45, 0.85, 4.18, -32.72]]\n",
      "[[8.72, 0.9, 4.19, -32.75], [8.73, 0.9, 4.21, -32.77], [8.8, 0.91, 4.23, -32.8], [8.65, 0.88, 4.25, -32.81], [8.64, 0.87, 4.27, -32.82], [8.54, 0.85, 4.29, -32.82], [8.5, 0.84, 4.31, -32.81], [8.31, 0.8, 4.32, -32.81], [8.29, 0.79, 4.33, -32.81], [8.29, 0.79, 4.34, -32.8]]\n",
      "[[8.24, 0.78, 4.35, -32.79], [8.19, 0.77, 4.36, -32.79], [8.15, 0.76, 4.36, -32.78], [8.11, 0.75, 4.37, -32.77], [8.03, 0.73, 4.37, -32.77], [7.95, 0.72, 4.37, -32.76], [7.93, 0.71, 4.37, -32.75], [7.91, 0.71, 4.37, -32.75], [7.83, 0.69, 4.37, -32.74], [7.81, 0.69, 4.37, -32.74]]\n",
      "[[7.76, 0.68, 4.37, -32.73], [7.69, 0.67, 4.37, -32.73], [7.67, 0.66, 4.36, -32.72], [7.63, 0.65, 4.36, -32.71], [7.58, 0.65, 4.35, -32.71], [7.55, 0.64, 4.35, -32.71], [7.52, 0.64, 4.34, -32.71], [7.49, 0.63, 4.33, -32.71], [7.46, 0.63, 4.32, -32.71], [7.51, 0.64, 4.32, -32.71]]\n",
      "[[7.59, 0.66, 4.31, -32.71], [7.53, 0.65, 4.3, -32.71], [7.47, 0.64, 4.29, -32.72], [7.43, 0.63, 4.29, -32.73], [7.44, 0.63, 4.28, -32.74], [7.37, 0.62, 4.27, -32.73], [7.37, 0.62, 4.26, -32.73], [7.3, 0.61, 4.26, -32.72], [7.32, 0.61, 4.25, -32.71], [7.67, 0.69, 4.24, -32.71]]\n",
      "[[8.04, 0.76, 4.23, -32.69], [8.04, 0.76, 4.23, -32.69], [7.86, 0.72, 4.24, -32.69], [7.85, 0.72, 4.24, -32.69], [7.68, 0.69, 4.24, -32.68], [7.66, 0.68, 4.25, -32.67], [7.59, 0.67, 4.25, -32.66], [7.61, 0.67, 4.26, -32.64], [7.53, 0.65, 4.26, -32.63], [7.52, 0.65, 4.26, -32.62]]\n",
      "[[7.44, 0.63, 4.26, -32.62], [7.48, 0.64, 4.26, -32.63], [7.4, 0.63, 4.26, -32.63], [7.41, 0.63, 4.26, -32.64], [7.34, 0.62, 4.25, -32.64], [7.37, 0.62, 4.25, -32.65], [7.29, 0.61, 4.25, -32.65], [7.23, 0.6, 4.25, -32.65], [7.28, 0.61, 4.24, -32.66], [7.16, 0.58, 4.24, -32.67]]\n",
      "[[7.14, 0.58, 4.24, -32.68], [7.16, 0.58, 4.23, -32.68], [7.16, 0.59, 4.23, -32.69], [7.15, 0.59, 4.23, -32.69], [7.11, 0.58, 4.22, -32.7], [7.07, 0.57, 4.22, -32.7], [7.06, 0.57, 4.22, -32.7], [7.02, 0.56, 4.21, -32.7], [7.04, 0.57, 4.21, -32.71], [7.42, 0.64, 4.21, -32.71]]\n",
      "[[7.63, 0.68, 4.21, -32.73], [7.79, 0.71, 4.22, -32.74], [7.89, 0.73, 4.22, -32.73], [7.86, 0.73, 4.23, -32.74], [7.71, 0.7, 4.24, -32.74], [7.79, 0.71, 4.24, -32.74], [7.71, 0.69, 4.24, -32.75], [7.59, 0.67, 4.25, -32.76], [7.45, 0.64, 4.26, -32.77], [7.59, 0.67, 4.26, -32.78]]\n",
      "[[7.41, 0.63, 4.26, -32.77], [7.41, 0.63, 4.27, -32.77], [7.39, 0.62, 4.27, -32.76], [7.24, 0.59, 4.27, -32.75], [7.28, 0.6, 4.27, -32.74], [8.14, 0.77, 4.27, -32.77], [7.94, 0.73, 4.28, -32.8], [8.23, 0.79, 4.29, -32.83], [8.11, 0.76, 4.31, -32.86], [8.43, 0.82, 4.32, -32.89]]\n",
      "[[8.24, 0.78, 4.33, -32.93], [8.22, 0.78, 4.34, -32.97], [7.96, 0.72, 4.35, -32.99], [7.92, 0.71, 4.36, -33.01], [7.94, 0.71, 4.38, -33.02], [7.8, 0.68, 4.38, -33.03], [7.85, 0.69, 4.39, -33.03], [7.7, 0.66, 4.39, -33.04], [7.66, 0.65, 4.4, -33.05], [7.62, 0.64, 4.4, -33.06]]\n",
      "[[7.6, 0.64, 4.4, -33.07], [7.5, 0.62, 4.4, -33.08], [7.45, 0.61, 4.4, -33.09], [7.42, 0.6, 4.4, -33.1], [7.37, 0.59, 4.4, -33.11], [7.34, 0.59, 4.39, -33.12], [7.29, 0.58, 4.39, -33.13], [7.27, 0.58, 4.39, -33.13], [7.24, 0.57, 4.38, -33.13], [7.21, 0.57, 4.37, -33.13]]\n",
      "[[7.17, 0.56, 4.37, -33.12], [7.14, 0.55, 4.37, -33.12], [7.1, 0.55, 4.36, -33.11], [7.1, 0.55, 4.36, -33.1], [7.06, 0.54, 4.35, -33.1], [7.01, 0.53, 4.34, -33.09], [6.98, 0.53, 4.33, -33.09], [6.96, 0.53, 4.33, -33.09], [6.94, 0.52, 4.32, -33.08], [6.91, 0.52, 4.31, -33.08]]\n",
      "[[6.89, 0.52, 4.3, -33.08], [6.87, 0.51, 4.3, -33.08], [6.84, 0.51, 4.29, -33.08], [6.81, 0.51, 4.28, -33.07], [6.79, 0.5, 4.28, -33.07], [6.8, 0.51, 4.27, -33.06], [6.96, 0.54, 4.27, -33.06], [6.99, 0.55, 4.27, -33.06], [7.06, 0.56, 4.26, -33.07], [7.05, 0.56, 4.26, -33.07]]\n",
      "[[7.2, 0.59, 4.26, -33.07], [6.95, 0.54, 4.25, -33.07], [6.99, 0.55, 4.25, -33.07], [6.98, 0.55, 4.25, -33.07], [6.88, 0.53, 4.25, -33.08], [6.98, 0.55, 4.25, -33.08], [6.87, 0.52, 4.25, -33.09], [6.88, 0.53, 4.25, -33.1], [6.87, 0.52, 4.25, -33.1], [6.84, 0.52, 4.25, -33.11]]\n",
      "[[6.8, 0.51, 4.25, -33.11], [6.73, 0.5, 4.25, -33.1], [6.77, 0.5, 4.25, -33.1], [6.8, 0.51, 4.25, -33.1], [6.82, 0.51, 4.25, -33.1], [6.67, 0.48, 4.25, -33.1], [6.73, 0.5, 4.25, -33.1], [6.7, 0.49, 4.25, -33.1], [6.69, 0.49, 4.24, -33.11], [6.67, 0.49, 4.24, -33.11]]\n",
      "[[6.65, 0.48, 4.24, -33.11], [6.62, 0.48, 4.24, -33.11], [6.59, 0.47, 4.23, -33.11], [6.59, 0.47, 4.23, -33.1], [6.58, 0.47, 4.23, -33.11], [6.6, 0.47, 4.23, -33.11], [6.51, 0.46, 4.23, -33.11], [6.44, 0.44, 4.23, -33.11], [6.47, 0.45, 4.23, -33.12], [6.69, 0.49, 4.23, -33.12]]\n",
      "[[6.91, 0.54, 4.23, -33.13], [7.16, 0.59, 4.23, -33.13], [6.9, 0.53, 4.23, -33.14], [7.02, 0.56, 4.24, -33.16], [7.01, 0.55, 4.24, -33.17], [6.93, 0.54, 4.25, -33.19], [7.13, 0.57, 4.26, -33.2], [6.85, 0.52, 4.26, -33.21], [7.04, 0.56, 4.27, -33.22], [7.07, 0.56, 4.27, -33.23]]\n",
      "[[6.99, 0.54, 4.28, -33.23], [7.12, 0.57, 4.29, -33.24], [7.03, 0.55, 4.3, -33.24], [6.98, 0.53, 4.3, -33.25], [6.81, 0.5, 4.31, -33.26], [6.8, 0.5, 4.31, -33.27], [6.95, 0.53, 4.32, -33.27], [6.75, 0.49, 4.32, -33.27], [6.81, 0.5, 4.33, -33.27], [6.66, 0.46, 4.34, -33.27]]\n",
      "[[7.0, 0.53, 4.34, -33.27], [6.98, 0.53, 4.34, -33.27], [6.94, 0.52, 4.35, -33.27], [6.89, 0.51, 4.35, -33.27], [6.86, 0.5, 4.35, -33.27], [6.9, 0.51, 4.36, -33.28], [6.82, 0.49, 4.36, -33.28], [6.83, 0.49, 4.36, -33.28], [6.89, 0.51, 4.36, -33.28], [6.81, 0.49, 4.37, -33.26]]\n",
      "[[6.73, 0.47, 4.37, -33.24], [6.67, 0.46, 4.37, -33.23], [6.68, 0.46, 4.37, -33.23], [6.66, 0.46, 4.37, -33.24], [6.58, 0.44, 4.37, -33.25], [6.57, 0.44, 4.37, -33.25], [6.49, 0.42, 4.37, -33.26], [6.47, 0.42, 4.37, -33.27], [6.41, 0.41, 4.37, -33.27], [6.4, 0.41, 4.37, -33.27]]\n",
      "[[6.4, 0.41, 4.37, -33.27], [6.4, 0.41, 4.37, -33.27], [6.42, 0.41, 4.36, -33.27], [6.47, 0.42, 4.35, -33.26], [6.61, 0.45, 4.35, -33.26], [6.64, 0.46, 4.35, -33.26], [6.48, 0.43, 4.34, -33.25], [7.9, 0.71, 4.35, -33.25], [7.09, 0.55, 4.36, -33.23], [6.9, 0.51, 4.37, -33.22]]\n",
      "[[7.03, 0.53, 4.38, -33.21], [7.14, 0.55, 4.39, -33.21], [7.35, 0.59, 4.41, -33.2], [7.06, 0.53, 4.42, -33.19], [6.93, 0.5, 4.43, -33.19], [6.87, 0.49, 4.44, -33.18], [6.92, 0.49, 4.45, -33.17], [6.82, 0.47, 4.45, -33.18], [6.74, 0.46, 4.46, -33.18], [6.67, 0.44, 4.47, -33.18]]\n",
      "[[6.58, 0.42, 4.47, -33.19], [6.6, 0.43, 4.47, -33.19], [6.59, 0.42, 4.47, -33.2], [6.48, 0.4, 4.47, -33.21], [6.47, 0.4, 4.47, -33.22], [6.53, 0.41, 4.47, -33.23], [6.51, 0.41, 4.47, -33.23], [6.54, 0.41, 4.47, -33.24], [6.47, 0.4, 4.47, -33.26], [6.49, 0.41, 4.46, -33.28]]\n",
      "[[6.41, 0.39, 4.45, -33.29], [6.55, 0.42, 4.45, -33.3], [6.5, 0.41, 4.44, -33.31], [6.52, 0.42, 4.44, -33.32], [6.86, 0.49, 4.43, -33.33], [6.72, 0.46, 4.43, -33.34], [7.16, 0.55, 4.43, -33.34], [6.85, 0.49, 4.42, -33.33], [6.79, 0.47, 4.43, -33.31], [8.26, 0.76, 4.45, -33.28]]\n",
      "[[9.55, 1.02, 4.47, -33.27], [9.8, 1.06, 4.51, -33.29], [10.03, 1.09, 4.56, -33.29], [9.62, 1.0, 4.61, -33.26], [10.74, 1.21, 4.67, -33.25], [9.56, 0.96, 4.73, -33.24], [10.44, 1.13, 4.79, -33.22], [11.09, 1.25, 4.84, -33.26], [11.94, 1.41, 4.89, -33.3], [12.1, 1.43, 4.94, -33.33]]\n",
      "[[11.55, 1.31, 5.0, -33.35], [10.92, 1.17, 5.06, -33.36], [10.81, 1.14, 5.13, -33.36], [10.55, 1.07, 5.18, -33.35], [10.44, 1.04, 5.23, -33.27], [10.49, 1.04, 5.28, -33.21], [10.48, 1.03, 5.32, -33.15], [10.31, 0.99, 5.36, -33.1], [10.24, 0.97, 5.4, -33.05], [10.13, 0.94, 5.43, -33.02]]\n",
      "[[10.07, 0.92, 5.45, -32.99], [9.98, 0.9, 5.47, -32.97], [9.86, 0.87, 5.49, -32.94], [9.85, 0.87, 5.51, -32.92], [9.84, 0.86, 5.53, -32.9], [9.77, 0.85, 5.54, -32.89], [9.74, 0.84, 5.55, -32.88], [9.67, 0.82, 5.55, -32.87], [9.52, 0.79, 5.55, -32.87], [9.37, 0.76, 5.55, -32.87]]\n",
      "[[9.31, 0.75, 5.55, -32.86], [9.25, 0.74, 5.54, -32.87], [9.18, 0.73, 5.54, -32.88], [9.08, 0.71, 5.53, -32.89], [8.98, 0.69, 5.52, -32.91], [8.9, 0.68, 5.51, -32.92], [8.89, 0.68, 5.49, -32.93], [8.87, 0.68, 5.48, -32.95], [8.78, 0.66, 5.47, -32.96], [8.67, 0.64, 5.46, -32.96]]\n",
      "[[8.61, 0.63, 5.45, -32.96], [8.57, 0.63, 5.44, -32.97], [8.5, 0.61, 5.43, -32.97], [8.47, 0.61, 5.41, -32.98], [8.38, 0.59, 5.4, -32.98], [8.32, 0.59, 5.39, -32.98], [8.26, 0.58, 5.38, -32.99], [8.24, 0.57, 5.37, -33.0], [8.2, 0.57, 5.36, -33.01], [8.15, 0.56, 5.35, -33.01]]\n",
      "[[8.09, 0.55, 5.33, -33.02], [8.02, 0.54, 5.32, -33.03], [7.97, 0.53, 5.3, -33.04], [7.93, 0.53, 5.29, -33.04], [7.88, 0.52, 5.28, -33.05], [7.83, 0.51, 5.27, -33.06], [7.8, 0.51, 5.26, -33.07], [7.78, 0.51, 5.25, -33.08], [7.75, 0.5, 5.24, -33.08], [7.72, 0.5, 5.23, -33.09]]\n",
      "[[7.65, 0.49, 5.22, -33.1], [7.63, 0.48, 5.21, -33.11], [7.6, 0.48, 5.2, -33.13], [7.54, 0.47, 5.18, -33.14], [7.5, 0.47, 5.17, -33.17], [7.47, 0.46, 5.16, -33.19], [7.44, 0.46, 5.15, -33.21], [7.42, 0.46, 5.14, -33.22], [7.38, 0.45, 5.13, -33.23], [7.39, 0.45, 5.12, -33.23]]\n",
      "[[7.55, 0.49, 5.12, -33.24], [7.53, 0.48, 5.11, -33.24], [7.5, 0.48, 5.1, -33.24], [7.44, 0.47, 5.1, -33.25], [7.46, 0.47, 5.09, -33.26], [7.37, 0.46, 5.08, -33.27], [7.32, 0.45, 5.07, -33.28], [7.25, 0.44, 5.06, -33.29], [7.27, 0.44, 5.05, -33.29], [7.27, 0.44, 5.05, -33.28]]\n",
      "[[7.28, 0.45, 5.04, -33.29], [7.3, 0.45, 5.03, -33.29], [7.37, 0.47, 5.03, -33.3], [7.27, 0.45, 5.02, -33.31], [7.19, 0.43, 5.01, -33.32], [7.3, 0.46, 5.01, -33.32], [7.25, 0.45, 5.0, -33.33], [7.17, 0.43, 5.0, -33.33], [7.22, 0.44, 4.99, -33.32], [7.05, 0.41, 4.99, -33.33]]\n",
      "[[7.1, 0.42, 4.98, -33.34], [7.03, 0.41, 4.98, -33.34], [7.04, 0.41, 4.97, -33.34], [7.04, 0.42, 4.96, -33.35], [7.01, 0.41, 4.96, -33.37], [6.98, 0.41, 4.95, -33.38], [6.94, 0.4, 4.95, -33.39], [6.9, 0.39, 4.94, -33.4], [6.87, 0.39, 4.94, -33.41], [6.93, 0.4, 4.93, -33.42]]\n",
      "[[6.91, 0.4, 4.93, -33.42], [6.88, 0.39, 4.92, -33.42], [6.82, 0.38, 4.91, -33.44], [6.8, 0.38, 4.9, -33.45], [6.76, 0.37, 4.9, -33.46], [6.73, 0.37, 4.89, -33.47], [6.71, 0.37, 4.88, -33.47], [6.7, 0.36, 4.88, -33.48], [6.67, 0.36, 4.87, -33.49], [6.63, 0.35, 4.86, -33.5]]\n",
      "[[6.6, 0.35, 4.85, -33.52], [6.58, 0.35, 4.85, -33.54], [6.57, 0.34, 4.84, -33.56], [6.54, 0.34, 4.84, -33.58], [6.52, 0.34, 4.83, -33.59], [6.51, 0.34, 4.82, -33.61], [6.5, 0.34, 4.82, -33.61], [6.47, 0.33, 4.81, -33.62], [6.44, 0.33, 4.81, -33.63], [6.44, 0.33, 4.8, -33.64]]\n",
      "[[6.42, 0.33, 4.79, -33.65], [6.4, 0.32, 4.79, -33.66], [6.39, 0.32, 4.78, -33.67], [6.37, 0.32, 4.77, -33.68], [6.36, 0.32, 4.77, -33.69], [6.34, 0.32, 4.76, -33.7], [6.32, 0.31, 4.76, -33.71], [6.32, 0.31, 4.75, -33.72], [6.32, 0.32, 4.74, -33.74], [6.33, 0.32, 4.74, -33.74]]\n",
      "[[6.36, 0.32, 4.73, -33.75], [6.29, 0.31, 4.73, -33.76], [6.29, 0.31, 4.72, -33.76], [6.32, 0.32, 4.72, -33.78], [6.33, 0.32, 4.71, -33.78], [6.44, 0.35, 4.71, -33.79], [6.74, 0.41, 4.7, -33.81], [6.54, 0.37, 4.7, -33.81], [6.63, 0.39, 4.7, -33.8], [6.46, 0.35, 4.7, -33.81]]\n",
      "[[6.51, 0.36, 4.69, -33.82], [6.49, 0.36, 4.69, -33.81], [6.47, 0.36, 4.69, -33.8], [6.42, 0.35, 4.68, -33.81], [6.37, 0.34, 4.68, -33.81], [6.39, 0.34, 4.68, -33.8], [6.34, 0.33, 4.68, -33.81], [6.31, 0.33, 4.68, -33.83], [6.27, 0.32, 4.68, -33.84], [6.28, 0.32, 4.68, -33.85]]\n",
      "[[6.22, 0.31, 4.67, -33.86], [6.22, 0.31, 4.67, -33.86], [6.21, 0.31, 4.67, -33.87], [6.16, 0.3, 4.66, -33.89], [6.16, 0.3, 4.66, -33.9], [6.16, 0.3, 4.65, -33.9], [6.13, 0.3, 4.65, -33.9], [6.15, 0.3, 4.64, -33.9], [6.11, 0.29, 4.64, -33.9], [6.12, 0.3, 4.63, -33.9]]\n",
      "[[6.1, 0.29, 4.63, -33.92], [6.04, 0.28, 4.63, -33.92], [6.05, 0.29, 4.62, -33.92], [6.3, 0.34, 4.61, -33.94], [6.43, 0.36, 4.61, -33.95], [6.34, 0.35, 4.61, -33.94], [6.27, 0.33, 4.61, -33.93], [6.27, 0.33, 4.61, -33.93], [6.77, 0.43, 4.61, -33.92], [8.01, 0.68, 4.61, -33.94]]\n",
      "[[8.41, 0.76, 4.62, -33.93], [7.66, 0.6, 4.63, -33.89], [8.02, 0.67, 4.65, -33.86], [8.01, 0.67, 4.67, -33.85], [7.97, 0.66, 4.68, -33.84], [7.81, 0.62, 4.69, -33.82], [7.59, 0.58, 4.71, -33.8], [7.48, 0.55, 4.72, -33.78], [7.36, 0.53, 4.73, -33.78], [7.24, 0.5, 4.74, -33.78]]\n",
      "[[7.13, 0.48, 4.75, -33.79], [7.04, 0.46, 4.76, -33.8], [6.97, 0.44, 4.77, -33.8], [7.17, 0.48, 4.78, -33.8], [7.17, 0.48, 4.78, -33.79], [7.17, 0.48, 4.78, -33.78], [7.16, 0.47, 4.79, -33.78], [7.09, 0.46, 4.79, -33.79], [7.03, 0.45, 4.79, -33.8], [6.92, 0.42, 4.79, -33.8]]\n",
      "[[6.86, 0.41, 4.8, -33.81], [6.78, 0.4, 4.8, -33.82], [6.74, 0.39, 4.8, -33.83], [6.66, 0.37, 4.8, -33.83], [6.64, 0.37, 4.8, -33.83], [6.62, 0.36, 4.79, -33.83], [6.58, 0.36, 4.79, -33.83], [6.53, 0.35, 4.78, -33.83], [6.47, 0.34, 4.78, -33.84], [6.42, 0.33, 4.77, -33.85]]\n",
      "[[6.43, 0.33, 4.76, -33.86], [6.38, 0.33, 4.76, -33.86], [6.32, 0.31, 4.75, -33.86], [6.28, 0.31, 4.74, -33.86], [6.3, 0.31, 4.74, -33.86], [6.24, 0.3, 4.73, -33.88], [6.24, 0.3, 4.72, -33.89], [6.27, 0.31, 4.71, -33.9], [6.23, 0.31, 4.7, -33.9], [6.2, 0.3, 4.7, -33.9]]\n",
      "[[6.19, 0.3, 4.69, -33.9], [6.17, 0.3, 4.68, -33.9], [6.15, 0.3, 4.67, -33.9], [6.13, 0.29, 4.67, -33.89], [6.1, 0.29, 4.66, -33.89], [6.08, 0.29, 4.65, -33.89], [6.06, 0.28, 4.64, -33.89], [6.05, 0.28, 4.64, -33.9], [6.02, 0.28, 4.63, -33.91], [6.03, 0.28, 4.62, -33.92]]\n",
      "[[5.99, 0.28, 4.62, -33.93], [5.98, 0.27, 4.61, -33.95], [5.97, 0.27, 4.6, -33.96], [5.99, 0.28, 4.59, -33.97], [5.93, 0.27, 4.59, -33.98], [6.0, 0.28, 4.58, -33.99], [5.96, 0.28, 4.57, -34.0], [5.93, 0.27, 4.56, -34.01], [5.89, 0.27, 4.56, -34.02], [5.9, 0.27, 4.56, -34.03]]\n",
      "[[5.92, 0.27, 4.55, -34.03], [5.9, 0.27, 4.55, -34.04], [5.88, 0.27, 4.54, -34.05], [5.88, 0.27, 4.53, -34.06], [5.86, 0.27, 4.53, -34.07], [5.85, 0.26, 4.52, -34.07], [5.83, 0.26, 4.52, -34.07], [5.81, 0.26, 4.51, -34.07], [5.79, 0.26, 4.51, -34.08], [5.78, 0.26, 4.5, -34.09]]\n",
      "[[5.79, 0.26, 4.5, -34.1], [5.77, 0.26, 4.49, -34.1], [5.76, 0.26, 4.48, -34.12], [5.75, 0.25, 4.48, -34.12], [5.75, 0.25, 4.47, -34.13], [5.73, 0.25, 4.47, -34.14], [5.72, 0.25, 4.47, -34.14], [5.71, 0.25, 4.46, -34.14], [5.73, 0.25, 4.46, -34.14], [5.76, 0.26, 4.46, -34.14]]\n",
      "[[5.77, 0.26, 4.45, -34.14], [5.78, 0.27, 4.45, -34.14], [5.7, 0.25, 4.44, -34.14], [5.68, 0.25, 4.44, -34.14], [5.7, 0.25, 4.44, -34.14], [5.66, 0.25, 4.43, -34.14], [5.68, 0.25, 4.43, -34.14], [5.69, 0.25, 4.43, -34.15], [5.7, 0.26, 4.42, -34.15], [5.83, 0.28, 4.42, -34.15]]\n",
      "[[6.36, 0.39, 4.42, -34.15], [7.18, 0.55, 4.41, -34.19], [7.11, 0.54, 4.42, -34.17], [6.86, 0.49, 4.43, -34.12], [6.61, 0.43, 4.43, -34.07], [6.7, 0.45, 4.44, -34.05], [6.35, 0.38, 4.45, -34.04], [6.36, 0.38, 4.46, -34.02], [6.47, 0.4, 4.47, -33.99], [6.2, 0.35, 4.47, -33.99]]\n",
      "[[6.26, 0.36, 4.47, -33.99], [6.22, 0.35, 4.48, -33.99], [6.24, 0.35, 4.48, -33.99], [6.19, 0.34, 4.48, -34.0], [6.13, 0.33, 4.48, -34.01], [6.1, 0.32, 4.49, -34.02], [6.07, 0.32, 4.49, -34.03], [5.98, 0.3, 4.49, -34.03], [5.99, 0.3, 4.49, -34.02], [5.94, 0.29, 4.5, -34.01]]\n",
      "[[5.91, 0.28, 4.5, -34.01], [5.95, 0.29, 4.49, -34.01], [5.9, 0.28, 4.49, -34.01], [5.91, 0.28, 4.49, -34.02], [5.86, 0.27, 4.48, -34.02], [5.81, 0.27, 4.48, -34.02], [5.83, 0.27, 4.48, -34.03], [5.8, 0.26, 4.48, -34.03], [5.77, 0.26, 4.47, -34.03], [5.74, 0.25, 4.47, -34.04]]\n",
      "[[5.73, 0.25, 4.47, -34.05], [5.71, 0.25, 4.46, -34.06], [5.68, 0.24, 4.46, -34.06], [5.68, 0.25, 4.45, -34.07], [5.65, 0.24, 4.45, -34.08], [5.65, 0.24, 4.44, -34.08], [5.63, 0.24, 4.43, -34.09], [5.61, 0.24, 4.43, -34.1], [5.6, 0.23, 4.42, -34.1], [5.58, 0.23, 4.42, -34.11]]\n",
      "[[5.57, 0.23, 4.41, -34.13], [5.56, 0.23, 4.41, -34.14], [5.55, 0.23, 4.4, -34.16], [5.54, 0.23, 4.39, -34.17], [5.53, 0.23, 4.39, -34.17], [5.52, 0.23, 4.38, -34.18], [5.51, 0.23, 4.38, -34.19], [5.5, 0.23, 4.37, -34.2], [5.49, 0.22, 4.37, -34.21], [5.49, 0.23, 4.36, -34.22]]\n",
      "[[5.5, 0.23, 4.36, -34.23], [5.53, 0.23, 4.36, -34.23], [5.69, 0.27, 4.35, -34.24], [5.58, 0.25, 4.35, -34.24], [5.64, 0.26, 4.35, -34.24], [5.87, 0.31, 4.34, -34.24], [6.08, 0.35, 4.34, -34.23], [5.97, 0.32, 4.35, -34.22], [5.83, 0.3, 4.35, -34.22], [6.1, 0.35, 4.35, -34.21]]\n",
      "[[6.25, 0.38, 4.35, -34.21], [6.1, 0.35, 4.35, -34.19], [6.05, 0.34, 4.35, -34.18], [5.85, 0.3, 4.36, -34.18], [5.98, 0.32, 4.36, -34.17], [5.91, 0.31, 4.36, -34.17], [5.83, 0.29, 4.37, -34.16], [5.87, 0.3, 4.37, -34.16], [5.83, 0.29, 4.38, -34.16], [5.77, 0.28, 4.38, -34.16]]\n",
      "[[5.72, 0.27, 4.38, -34.17], [5.71, 0.27, 4.38, -34.18], [5.68, 0.26, 4.38, -34.19], [5.68, 0.26, 4.37, -34.19], [5.65, 0.26, 4.37, -34.2], [5.62, 0.25, 4.37, -34.21], [5.58, 0.24, 4.37, -34.21], [5.56, 0.24, 4.36, -34.22], [5.57, 0.24, 4.36, -34.21], [5.56, 0.24, 4.36, -34.21]]\n",
      "[[5.51, 0.23, 4.36, -34.21], [5.54, 0.24, 4.36, -34.22], [5.52, 0.23, 4.35, -34.22], [5.5, 0.23, 4.35, -34.23], [5.5, 0.23, 4.35, -34.24], [5.47, 0.22, 4.35, -34.24], [5.45, 0.22, 4.34, -34.24], [5.46, 0.22, 4.34, -34.25], [5.44, 0.22, 4.34, -34.25], [5.41, 0.21, 4.34, -34.26]]\n",
      "[[5.43, 0.22, 4.33, -34.26], [5.4, 0.22, 4.33, -34.26], [5.39, 0.21, 4.32, -34.27], [5.42, 0.22, 4.32, -34.27], [5.41, 0.22, 4.31, -34.27], [5.36, 0.21, 4.31, -34.28], [5.43, 0.23, 4.3, -34.29], [5.46, 0.23, 4.3, -34.29], [5.38, 0.22, 4.29, -34.29], [5.46, 0.23, 4.29, -34.3]]\n",
      "[[5.42, 0.23, 4.29, -34.29], [5.46, 0.23, 4.28, -34.29], [5.34, 0.21, 4.28, -34.29], [5.5, 0.24, 4.28, -34.3], [5.65, 0.28, 4.28, -34.29], [6.02, 0.35, 4.27, -34.29], [5.65, 0.28, 4.27, -34.3], [5.68, 0.28, 4.27, -34.3], [5.98, 0.34, 4.27, -34.3], [5.64, 0.27, 4.28, -34.3]]\n",
      "[[5.78, 0.3, 4.28, -34.29], [5.72, 0.29, 4.28, -34.27], [5.72, 0.29, 4.29, -34.26], [5.7, 0.28, 4.29, -34.26], [5.53, 0.25, 4.29, -34.24], [5.59, 0.26, 4.29, -34.23], [5.52, 0.25, 4.29, -34.23], [5.54, 0.25, 4.29, -34.23], [5.48, 0.24, 4.29, -34.22], [5.5, 0.24, 4.29, -34.22]]\n",
      "[[5.44, 0.23, 4.29, -34.22], [5.46, 0.23, 4.29, -34.23], [5.44, 0.23, 4.29, -34.23], [5.4, 0.22, 4.28, -34.24], [5.39, 0.22, 4.28, -34.25], [5.36, 0.22, 4.28, -34.27], [5.37, 0.22, 4.27, -34.28], [5.34, 0.21, 4.27, -34.29], [5.34, 0.21, 4.27, -34.29], [5.32, 0.21, 4.27, -34.3]]\n",
      "[[5.31, 0.21, 4.26, -34.3], [5.29, 0.21, 4.26, -34.3], [5.29, 0.21, 4.26, -34.29], [5.27, 0.2, 4.25, -34.29], [5.28, 0.21, 4.25, -34.3], [5.26, 0.2, 4.25, -34.3], [5.25, 0.2, 4.24, -34.3], [5.25, 0.2, 4.24, -34.31], [5.24, 0.2, 4.23, -34.32], [5.22, 0.2, 4.23, -34.32]]\n",
      "[[5.23, 0.2, 4.23, -34.33], [5.22, 0.2, 4.22, -34.33], [5.21, 0.2, 4.22, -34.33], [5.21, 0.2, 4.22, -34.34], [5.2, 0.2, 4.21, -34.34], [5.23, 0.2, 4.21, -34.34], [5.24, 0.21, 4.21, -34.35], [5.34, 0.23, 4.2, -34.35], [5.31, 0.22, 4.2, -34.36], [5.29, 0.22, 4.2, -34.37]]\n",
      "[[5.25, 0.21, 4.19, -34.37], [5.3, 0.22, 4.19, -34.36], [5.34, 0.23, 4.19, -34.37], [5.37, 0.23, 4.19, -34.38], [5.28, 0.22, 4.19, -34.37], [5.37, 0.24, 4.19, -34.36], [5.3, 0.22, 4.19, -34.36], [5.43, 0.25, 4.18, -34.37], [5.62, 0.29, 4.18, -34.36], [5.79, 0.32, 4.18, -34.36]]\n",
      "[[5.64, 0.29, 4.19, -34.36], [5.71, 0.3, 4.19, -34.37], [5.54, 0.27, 4.19, -34.37], [5.55, 0.27, 4.19, -34.35], [5.55, 0.27, 4.2, -34.35], [5.39, 0.24, 4.2, -34.35], [5.45, 0.25, 4.2, -34.34], [5.35, 0.23, 4.2, -34.34], [5.35, 0.23, 4.21, -34.34], [5.37, 0.23, 4.21, -34.35]]\n",
      "[[5.31, 0.22, 4.21, -34.36], [5.3, 0.22, 4.21, -34.37], [5.3, 0.22, 4.2, -34.37], [5.27, 0.21, 4.2, -34.37], [5.26, 0.21, 4.2, -34.36], [5.24, 0.21, 4.2, -34.35], [5.22, 0.2, 4.2, -34.35], [5.21, 0.2, 4.2, -34.35], [5.21, 0.2, 4.2, -34.35], [5.22, 0.2, 4.2, -34.36]]\n",
      "[[5.24, 0.21, 4.2, -34.36], [5.19, 0.2, 4.19, -34.36], [5.18, 0.2, 4.19, -34.36], [5.22, 0.21, 4.19, -34.35], [5.18, 0.2, 4.19, -34.34], [5.15, 0.19, 4.18, -34.34], [5.16, 0.2, 4.18, -34.34], [5.14, 0.19, 4.18, -34.34], [5.12, 0.19, 4.18, -34.35], [5.12, 0.19, 4.17, -34.36]]\n",
      "[[5.11, 0.19, 4.17, -34.36], [5.09, 0.18, 4.17, -34.36], [5.09, 0.19, 4.16, -34.36], [5.08, 0.18, 4.16, -34.36], [5.07, 0.18, 4.16, -34.36], [5.08, 0.18, 4.15, -34.36], [5.07, 0.18, 4.15, -34.37], [5.13, 0.2, 4.15, -34.37], [5.1, 0.19, 4.14, -34.38], [5.13, 0.2, 4.14, -34.38]]\n",
      "[[5.07, 0.19, 4.14, -34.39], [5.11, 0.2, 4.14, -34.39], [5.24, 0.22, 4.13, -34.38], [5.17, 0.21, 4.13, -34.38], [5.24, 0.22, 4.13, -34.39], [5.2, 0.21, 4.13, -34.38], [5.21, 0.22, 4.13, -34.38], [5.13, 0.2, 4.13, -34.39], [5.26, 0.23, 4.13, -34.39], [5.71, 0.31, 4.14, -34.39]]\n",
      "[[6.52, 0.48, 4.14, -34.39], [6.07, 0.38, 4.15, -34.4], [6.28, 0.43, 4.16, -34.38], [6.3, 0.43, 4.16, -34.36], [6.52, 0.47, 4.17, -34.35], [6.37, 0.44, 4.18, -34.33], [6.08, 0.38, 4.2, -34.31], [5.82, 0.32, 4.22, -34.29], [6.1, 0.37, 4.23, -34.27], [5.9, 0.33, 4.25, -34.25]]\n",
      "[[5.99, 0.35, 4.26, -34.23], [5.91, 0.33, 4.27, -34.21], [5.81, 0.31, 4.28, -34.19], [5.82, 0.31, 4.29, -34.18], [5.73, 0.28, 4.3, -34.15], [5.71, 0.28, 4.31, -34.13], [5.73, 0.28, 4.32, -34.11], [5.68, 0.27, 4.33, -34.1], [5.6, 0.25, 4.33, -34.1], [5.56, 0.24, 4.34, -34.11]]\n",
      "[[5.56, 0.24, 4.34, -34.12], [5.49, 0.23, 4.34, -34.13], [5.5, 0.23, 4.34, -34.14], [5.45, 0.22, 4.34, -34.14], [5.43, 0.22, 4.33, -34.14], [5.4, 0.21, 4.33, -34.13], [5.36, 0.21, 4.33, -34.13], [5.35, 0.2, 4.32, -34.13], [5.34, 0.2, 4.32, -34.13], [5.3, 0.2, 4.31, -34.13]]\n",
      "[[5.27, 0.19, 4.31, -34.14], [5.26, 0.19, 4.3, -34.14], [5.23, 0.19, 4.3, -34.15], [5.22, 0.19, 4.29, -34.15], [5.21, 0.18, 4.29, -34.16], [5.19, 0.18, 4.28, -34.17], [5.18, 0.18, 4.27, -34.18], [5.15, 0.18, 4.27, -34.19], [5.15, 0.18, 4.26, -34.2], [5.16, 0.18, 4.25, -34.21]]\n",
      "[[5.16, 0.18, 4.25, -34.21], [5.13, 0.18, 4.24, -34.22], [5.14, 0.18, 4.23, -34.23], [5.12, 0.18, 4.23, -34.23], [5.11, 0.18, 4.22, -34.23], [5.09, 0.17, 4.22, -34.23], [5.1, 0.18, 4.21, -34.24], [5.07, 0.17, 4.2, -34.24], [5.05, 0.17, 4.2, -34.24], [5.05, 0.17, 4.19, -34.24]]\n",
      "[[5.04, 0.17, 4.19, -34.25], [5.07, 0.18, 4.18, -34.25], [5.06, 0.18, 4.17, -34.26], [5.06, 0.18, 4.17, -34.26], [5.05, 0.18, 4.16, -34.26], [5.0, 0.17, 4.16, -34.27], [5.05, 0.18, 4.15, -34.28], [5.02, 0.17, 4.15, -34.28], [5.01, 0.17, 4.14, -34.29], [4.99, 0.17, 4.14, -34.29]]\n",
      "[[5.03, 0.18, 4.14, -34.3], [5.01, 0.18, 4.13, -34.31], [4.99, 0.17, 4.13, -34.31], [5.02, 0.18, 4.13, -34.31], [4.97, 0.17, 4.12, -34.32], [4.99, 0.17, 4.12, -34.33], [5.04, 0.19, 4.12, -34.33], [5.02, 0.18, 4.11, -34.33], [5.06, 0.19, 4.11, -34.34], [5.04, 0.19, 4.11, -34.34]]\n",
      "[[4.96, 0.17, 4.11, -34.35], [5.06, 0.19, 4.1, -34.36], [5.32, 0.24, 4.1, -34.35], [5.2, 0.22, 4.1, -34.35], [5.14, 0.21, 4.11, -34.35], [5.14, 0.21, 4.1, -34.34], [5.11, 0.2, 4.1, -34.34], [5.24, 0.23, 4.1, -34.34], [5.2, 0.22, 4.1, -34.34], [5.04, 0.19, 4.11, -34.34]]\n",
      "[[5.2, 0.22, 4.11, -34.32], [5.2, 0.22, 4.11, -34.33], [5.23, 0.22, 4.11, -34.33], [5.16, 0.21, 4.12, -34.32], [5.23, 0.22, 4.12, -34.32], [5.16, 0.21, 4.11, -34.32], [5.2, 0.22, 4.11, -34.32], [5.21, 0.22, 4.11, -34.33], [5.23, 0.22, 4.11, -34.33], [5.12, 0.2, 4.11, -34.32]]\n",
      "[[5.1, 0.2, 4.12, -34.32], [5.1, 0.2, 4.12, -34.32], [5.07, 0.19, 4.12, -34.32], [5.07, 0.19, 4.12, -34.33], [5.06, 0.19, 4.12, -34.32], [5.05, 0.19, 4.12, -34.32], [5.01, 0.18, 4.12, -34.33], [5.01, 0.18, 4.12, -34.33], [5.0, 0.18, 4.11, -34.33], [4.98, 0.17, 4.11, -34.34]]\n",
      "[[4.98, 0.17, 4.11, -34.34], [4.96, 0.17, 4.11, -34.35], [4.95, 0.17, 4.1, -34.36], [4.95, 0.17, 4.1, -34.36], [4.92, 0.16, 4.1, -34.36], [4.91, 0.16, 4.1, -34.35], [4.93, 0.17, 4.09, -34.35], [4.96, 0.17, 4.09, -34.36], [4.94, 0.17, 4.09, -34.37], [4.9, 0.16, 4.08, -34.37]]\n",
      "[[4.97, 0.18, 4.08, -34.37], [4.89, 0.16, 4.08, -34.38], [4.98, 0.18, 4.07, -34.39], [4.96, 0.18, 4.07, -34.39], [4.9, 0.17, 4.07, -34.39], [4.96, 0.18, 4.07, -34.39], [5.15, 0.22, 4.07, -34.39], [5.27, 0.24, 4.06, -34.41], [4.97, 0.18, 4.06, -34.4], [5.35, 0.26, 4.07, -34.4]]\n",
      "[[5.85, 0.36, 4.07, -34.39], [6.4, 0.47, 4.07, -34.4], [6.13, 0.41, 4.09, -34.4], [6.03, 0.39, 4.1, -34.37], [5.7, 0.32, 4.11, -34.39], [5.95, 0.37, 4.12, -34.4], [5.55, 0.28, 4.13, -34.38], [5.83, 0.34, 4.15, -34.36], [5.71, 0.31, 4.16, -34.36], [5.97, 0.36, 4.17, -34.35]]\n",
      "[[6.02, 0.37, 4.18, -34.33], [6.57, 0.47, 4.2, -34.31], [7.23, 0.6, 4.21, -34.29], [7.95, 0.74, 4.22, -34.31], [6.86, 0.52, 4.24, -34.28], [8.11, 0.77, 4.27, -34.19], [7.78, 0.7, 4.29, -34.14], [7.56, 0.65, 4.32, -34.13], [7.89, 0.71, 4.35, -34.13], [7.23, 0.57, 4.38, -34.13]]\n",
      "[[7.18, 0.55, 4.41, -34.11], [7.01, 0.51, 4.44, -34.07], [6.88, 0.48, 4.47, -34.02], [12.9, 1.68, 4.48, -33.97], [11.3, 1.36, 4.52, -33.88], [11.84, 1.46, 4.56, -33.8], [11.7, 1.42, 4.61, -33.72], [11.47, 1.36, 4.67, -33.66], [11.3, 1.32, 4.72, -33.61], [10.87, 1.22, 4.77, -33.56]]\n",
      "[[10.65, 1.17, 4.82, -33.52], [10.39, 1.1, 4.87, -33.48], [10.27, 1.07, 4.91, -33.45], [10.09, 1.03, 4.95, -33.42], [10.01, 1.01, 4.98, -33.39], [9.87, 0.97, 5.02, -33.37], [9.75, 0.94, 5.04, -33.35], [9.62, 0.91, 5.07, -33.33], [9.52, 0.89, 5.09, -33.31], [9.38, 0.86, 5.1, -33.29]]\n",
      "[[9.32, 0.84, 5.12, -33.28], [9.2, 0.82, 5.13, -33.27], [9.14, 0.8, 5.14, -33.26], [9.23, 0.82, 5.14, -33.26], [9.14, 0.8, 5.15, -33.27], [8.99, 0.77, 5.15, -33.29], [8.89, 0.75, 5.15, -33.31], [8.79, 0.73, 5.15, -33.33], [8.67, 0.7, 5.15, -33.36], [8.58, 0.69, 5.15, -33.38]]\n",
      "[[8.49, 0.67, 5.14, -33.39], [8.38, 0.65, 5.14, -33.4], [8.28, 0.63, 5.14, -33.41], [8.23, 0.62, 5.14, -33.42], [8.39, 0.65, 5.13, -33.42], [8.48, 0.67, 5.13, -33.42], [8.34, 0.64, 5.12, -33.43], [8.16, 0.61, 5.12, -33.44], [8.08, 0.59, 5.12, -33.44], [8.16, 0.61, 5.11, -33.44]]\n",
      "[[8.46, 0.67, 5.11, -33.44], [8.32, 0.64, 5.11, -33.43], [8.35, 0.65, 5.1, -33.43], [8.27, 0.63, 5.1, -33.43], [8.13, 0.61, 5.1, -33.43], [7.96, 0.57, 5.1, -33.45], [7.87, 0.55, 5.1, -33.46], [7.82, 0.54, 5.1, -33.47], [7.71, 0.52, 5.1, -33.48], [7.59, 0.5, 5.1, -33.49]]\n",
      "[[7.52, 0.48, 5.1, -33.5], [7.46, 0.47, 5.1, -33.51], [7.37, 0.46, 5.09, -33.53], [7.28, 0.44, 5.09, -33.54], [7.21, 0.43, 5.08, -33.55], [7.16, 0.42, 5.07, -33.56], [7.04, 0.4, 5.07, -33.56], [6.99, 0.39, 5.06, -33.56], [6.89, 0.37, 5.05, -33.56], [6.83, 0.36, 5.05, -33.56]]\n",
      "[[6.78, 0.35, 5.04, -33.55], [6.74, 0.34, 5.03, -33.55], [6.7, 0.34, 5.02, -33.55], [6.63, 0.32, 5.01, -33.55], [6.61, 0.32, 5.0, -33.56], [6.55, 0.31, 4.99, -33.57], [6.52, 0.31, 4.97, -33.57], [6.46, 0.3, 4.96, -33.58], [6.44, 0.3, 4.95, -33.58], [6.37, 0.29, 4.94, -33.59]]\n",
      "[[6.45, 0.31, 4.93, -33.59], [6.41, 0.3, 4.92, -33.59], [6.46, 0.31, 4.91, -33.59], [6.34, 0.29, 4.89, -33.59], [6.33, 0.29, 4.88, -33.6], [6.28, 0.28, 4.87, -33.61], [6.34, 0.3, 4.86, -33.61], [6.4, 0.31, 4.85, -33.61], [6.21, 0.27, 4.84, -33.62], [6.32, 0.3, 4.83, -33.62]]\n",
      "[[6.18, 0.27, 4.83, -33.62], [6.14, 0.26, 4.82, -33.61], [6.22, 0.28, 4.81, -33.61], [6.08, 0.26, 4.8, -33.61], [6.11, 0.26, 4.79, -33.61], [6.11, 0.27, 4.78, -33.62], [6.1, 0.27, 4.77, -33.62], [6.03, 0.25, 4.77, -33.62], [6.27, 0.3, 4.76, -33.63], [6.36, 0.32, 4.75, -33.64]]\n",
      "[[6.52, 0.35, 4.75, -33.63], [6.74, 0.4, 4.74, -33.65], [6.98, 0.45, 4.73, -33.67], [7.26, 0.5, 4.74, -33.67], [6.71, 0.39, 4.74, -33.66], [6.84, 0.42, 4.74, -33.65], [6.84, 0.42, 4.75, -33.64], [6.79, 0.41, 4.76, -33.63], [6.54, 0.35, 4.77, -33.61], [6.43, 0.33, 4.78, -33.59]]\n",
      "[[6.39, 0.32, 4.78, -33.58], [6.37, 0.32, 4.79, -33.57], [6.38, 0.32, 4.79, -33.56], [6.46, 0.34, 4.78, -33.55], [6.72, 0.39, 4.78, -33.54], [6.78, 0.4, 4.78, -33.53], [6.7, 0.38, 4.77, -33.54], [6.63, 0.37, 4.77, -33.54], [6.5, 0.35, 4.77, -33.54], [6.34, 0.31, 4.77, -33.55]]\n",
      "[[6.36, 0.32, 4.77, -33.56], [6.49, 0.34, 4.77, -33.57], [6.31, 0.31, 4.77, -33.59], [6.4, 0.33, 4.77, -33.61], [6.43, 0.33, 4.77, -33.62], [6.87, 0.42, 4.76, -33.62], [6.64, 0.37, 4.76, -33.61], [6.6, 0.37, 4.76, -33.61], [6.69, 0.39, 4.76, -33.59], [6.68, 0.38, 4.76, -33.59]]\n",
      "[[6.48, 0.34, 4.76, -33.58], [6.4, 0.33, 4.76, -33.58], [6.43, 0.34, 4.76, -33.58], [6.36, 0.32, 4.75, -33.59], [6.25, 0.3, 4.75, -33.59], [6.18, 0.28, 4.75, -33.58], [6.13, 0.28, 4.75, -33.58], [6.13, 0.28, 4.75, -33.58], [6.04, 0.26, 4.74, -33.58], [5.97, 0.25, 4.74, -33.59]]\n",
      "[[5.95, 0.24, 4.73, -33.6], [5.98, 0.25, 4.73, -33.61], [5.9, 0.24, 4.72, -33.61], [5.9, 0.24, 4.72, -33.62], [5.92, 0.24, 4.72, -33.62], [5.84, 0.22, 4.71, -33.63], [5.82, 0.22, 4.71, -33.63], [5.76, 0.21, 4.7, -33.64], [5.71, 0.2, 4.7, -33.64], [5.74, 0.21, 4.69, -33.65]]\n",
      "[[5.67, 0.2, 4.69, -33.66], [5.65, 0.19, 4.68, -33.67], [5.63, 0.19, 4.67, -33.68], [5.59, 0.19, 4.67, -33.68], [5.62, 0.19, 4.66, -33.69], [5.56, 0.18, 4.65, -33.69], [5.58, 0.19, 4.64, -33.69], [5.57, 0.19, 4.64, -33.69], [5.54, 0.18, 4.63, -33.69], [5.53, 0.18, 4.62, -33.69]]\n",
      "[[5.49, 0.17, 4.62, -33.69], [5.49, 0.18, 4.61, -33.7], [5.49, 0.18, 4.6, -33.7], [5.47, 0.18, 4.59, -33.7], [5.46, 0.17, 4.59, -33.7], [5.41, 0.17, 4.58, -33.7], [5.44, 0.17, 4.57, -33.71], [5.39, 0.17, 4.56, -33.71], [5.4, 0.17, 4.55, -33.72], [5.39, 0.17, 4.54, -33.73]]\n",
      "[[5.4, 0.17, 4.54, -33.74], [5.34, 0.16, 4.53, -33.74], [5.35, 0.16, 4.53, -33.74], [5.34, 0.16, 4.52, -33.74], [5.32, 0.16, 4.51, -33.73], [5.31, 0.16, 4.5, -33.73], [5.28, 0.16, 4.5, -33.74], [5.31, 0.16, 4.49, -33.75], [5.28, 0.16, 4.48, -33.76], [5.36, 0.18, 4.48, -33.76]]\n",
      "[[5.39, 0.18, 4.47, -33.76], [5.41, 0.19, 4.47, -33.76], [5.38, 0.18, 4.46, -33.76], [5.31, 0.17, 4.46, -33.77], [5.3, 0.17, 4.45, -33.77], [5.28, 0.17, 4.45, -33.78], [5.28, 0.17, 4.44, -33.79], [5.27, 0.17, 4.43, -33.79], [5.24, 0.16, 4.43, -33.79], [5.21, 0.16, 4.43, -33.78]]\n",
      "[[5.21, 0.16, 4.42, -33.78], [5.18, 0.15, 4.42, -33.78], [5.18, 0.15, 4.41, -33.79], [5.17, 0.15, 4.41, -33.79], [5.16, 0.15, 4.4, -33.79], [5.15, 0.15, 4.4, -33.79], [5.13, 0.15, 4.39, -33.79], [5.14, 0.15, 4.39, -33.8], [5.21, 0.17, 4.38, -33.8], [5.18, 0.16, 4.38, -33.8]]\n",
      "[[5.22, 0.17, 4.37, -33.81], [5.17, 0.16, 4.37, -33.81], [5.19, 0.16, 4.37, -33.81], [5.15, 0.16, 4.36, -33.81], [5.14, 0.16, 4.36, -33.82], [5.16, 0.16, 4.36, -33.82], [5.11, 0.15, 4.35, -33.82], [5.1, 0.15, 4.34, -33.82], [5.11, 0.15, 4.34, -33.82], [5.08, 0.15, 4.34, -33.82]]\n",
      "[[5.08, 0.15, 4.33, -33.81], [5.09, 0.15, 4.33, -33.8], [5.06, 0.15, 4.33, -33.8], [5.06, 0.15, 4.32, -33.8], [5.04, 0.14, 4.32, -33.8], [5.03, 0.14, 4.31, -33.8], [5.01, 0.14, 4.31, -33.81], [5.03, 0.14, 4.3, -33.81], [5.02, 0.14, 4.3, -33.81], [5.04, 0.15, 4.3, -33.81]]\n",
      "[[5.01, 0.14, 4.29, -33.81], [5.04, 0.15, 4.29, -33.81], [5.02, 0.15, 4.29, -33.81], [5.03, 0.15, 4.28, -33.81], [5.0, 0.14, 4.28, -33.81], [4.98, 0.14, 4.27, -33.82], [4.98, 0.14, 4.27, -33.82], [4.97, 0.14, 4.27, -33.82], [4.96, 0.14, 4.26, -33.82], [4.95, 0.14, 4.26, -33.82]]\n",
      "[[4.97, 0.14, 4.26, -33.82], [4.96, 0.14, 4.25, -33.82], [5.09, 0.17, 4.25, -33.82], [5.09, 0.17, 4.24, -33.82], [5.02, 0.16, 4.24, -33.83], [5.07, 0.17, 4.24, -33.83], [5.08, 0.17, 4.23, -33.83], [5.04, 0.16, 4.23, -33.83], [5.02, 0.16, 4.23, -33.83], [5.06, 0.17, 4.23, -33.83]]\n",
      "[[5.36, 0.23, 4.23, -33.83], [5.44, 0.24, 4.23, -33.83], [5.77, 0.31, 4.23, -33.84], [6.48, 0.45, 4.23, -33.82], [6.92, 0.54, 4.24, -33.8], [6.11, 0.37, 4.26, -33.78], [6.87, 0.52, 4.28, -33.76], [6.61, 0.46, 4.3, -33.74], [6.55, 0.45, 4.32, -33.73], [6.34, 0.4, 4.34, -33.72]]\n",
      "[[6.22, 0.37, 4.36, -33.71], [6.3, 0.39, 4.37, -33.69], [7.52, 0.63, 4.38, -33.69], [7.22, 0.57, 4.39, -33.67], [7.81, 0.68, 4.42, -33.65], [7.38, 0.59, 4.45, -33.64], [7.36, 0.58, 4.49, -33.63], [7.32, 0.56, 4.52, -33.62], [7.13, 0.52, 4.55, -33.62], [6.99, 0.48, 4.57, -33.61]]\n",
      "[[6.68, 0.42, 4.59, -33.6], [6.51, 0.38, 4.61, -33.57], [6.39, 0.35, 4.63, -33.55], [6.32, 0.34, 4.65, -33.53], [6.3, 0.33, 4.66, -33.52], [6.27, 0.32, 4.67, -33.51], [6.19, 0.3, 4.68, -33.51], [6.13, 0.29, 4.68, -33.5], [6.11, 0.29, 4.69, -33.5], [6.01, 0.26, 4.69, -33.49]]\n",
      "[[5.93, 0.25, 4.69, -33.48], [5.95, 0.25, 4.69, -33.46], [5.89, 0.24, 4.69, -33.45], [5.93, 0.25, 4.69, -33.44], [5.84, 0.23, 4.69, -33.43], [5.8, 0.22, 4.68, -33.42], [5.81, 0.23, 4.68, -33.42], [5.73, 0.21, 4.68, -33.42], [5.68, 0.2, 4.67, -33.43], [5.63, 0.19, 4.67, -33.43]]\n",
      "[[5.59, 0.18, 4.66, -33.43], [5.55, 0.18, 4.66, -33.43], [5.52, 0.17, 4.65, -33.43], [5.5, 0.17, 4.65, -33.44], [5.46, 0.16, 4.64, -33.45], [5.44, 0.16, 4.63, -33.46], [5.42, 0.16, 4.62, -33.46], [5.4, 0.16, 4.61, -33.47], [5.4, 0.16, 4.61, -33.47], [5.38, 0.16, 4.6, -33.46]]\n",
      "[[5.35, 0.15, 4.59, -33.46], [5.33, 0.15, 4.58, -33.47], [5.32, 0.15, 4.56, -33.49], [5.32, 0.15, 4.55, -33.5], [5.32, 0.16, 4.54, -33.51], [5.26, 0.15, 4.53, -33.54], [5.26, 0.15, 4.52, -33.56], [5.22, 0.14, 4.5, -33.58], [5.24, 0.15, 4.49, -33.59], [5.2, 0.14, 4.48, -33.6]]\n",
      "[[5.18, 0.14, 4.48, -33.61], [5.15, 0.14, 4.47, -33.61], [5.14, 0.14, 4.46, -33.62], [5.12, 0.13, 4.45, -33.62], [5.11, 0.13, 4.44, -33.63], [5.08, 0.13, 4.43, -33.64], [5.08, 0.13, 4.42, -33.65], [5.05, 0.13, 4.42, -33.66], [5.05, 0.13, 4.41, -33.67], [5.02, 0.12, 4.4, -33.67]]\n",
      "[[5.06, 0.13, 4.39, -33.68], [5.03, 0.13, 4.38, -33.68], [5.01, 0.13, 4.37, -33.69], [5.02, 0.13, 4.37, -33.7], [4.99, 0.13, 4.36, -33.7], [4.99, 0.13, 4.35, -33.7], [4.98, 0.13, 4.34, -33.7], [4.95, 0.12, 4.34, -33.71], [4.94, 0.12, 4.33, -33.72], [4.93, 0.12, 4.32, -33.72]]\n",
      "[[4.92, 0.12, 4.31, -33.73], [4.9, 0.12, 4.31, -33.74], [4.92, 0.12, 4.3, -33.75], [4.9, 0.12, 4.29, -33.75], [4.91, 0.12, 4.28, -33.76], [4.88, 0.12, 4.28, -33.77], [4.88, 0.12, 4.27, -33.77], [4.87, 0.12, 4.26, -33.78], [4.85, 0.12, 4.26, -33.79], [4.84, 0.12, 4.25, -33.8]]\n",
      "[[4.84, 0.12, 4.25, -33.8], [4.83, 0.12, 4.24, -33.81], [4.82, 0.12, 4.23, -33.81], [4.81, 0.12, 4.23, -33.82], [4.85, 0.12, 4.22, -33.82], [4.91, 0.14, 4.22, -33.82], [6.29, 0.41, 4.22, -33.79], [6.94, 0.55, 4.22, -33.73], [8.78, 0.91, 4.23, -33.68], [7.97, 0.74, 4.26, -33.63]]\n",
      "[[7.69, 0.68, 4.3, -33.59], [7.58, 0.65, 4.33, -33.55], [7.53, 0.63, 4.36, -33.52], [7.51, 0.62, 4.39, -33.51], [7.42, 0.6, 4.42, -33.49], [7.21, 0.55, 4.45, -33.46], [7.27, 0.56, 4.48, -33.43], [7.09, 0.52, 4.51, -33.4], [6.85, 0.46, 4.54, -33.37], [6.85, 0.46, 4.56, -33.35]]\n",
      "[[6.58, 0.4, 4.59, -33.33], [6.64, 0.41, 4.61, -33.31], [6.79, 0.43, 4.64, -33.27], [6.94, 0.46, 4.66, -33.23], [6.94, 0.45, 4.68, -33.21], [6.83, 0.43, 4.7, -33.19], [6.85, 0.43, 4.72, -33.18], [6.59, 0.37, 4.73, -33.17], [6.6, 0.37, 4.74, -33.16], [6.39, 0.33, 4.74, -33.14]]\n",
      "[[6.55, 0.36, 4.75, -33.14], [6.4, 0.33, 4.76, -33.14], [6.32, 0.31, 4.76, -33.14], [6.23, 0.29, 4.77, -33.14], [6.18, 0.28, 4.77, -33.15], [6.14, 0.27, 4.77, -33.15], [6.06, 0.26, 4.77, -33.16], [5.96, 0.24, 4.78, -33.16], [5.99, 0.24, 4.78, -33.17], [5.92, 0.23, 4.78, -33.17]]\n",
      "[[5.86, 0.22, 4.77, -33.18], [5.84, 0.21, 4.77, -33.19], [5.78, 0.2, 4.76, -33.2], [5.72, 0.19, 4.76, -33.21], [5.72, 0.19, 4.75, -33.22], [5.67, 0.19, 4.74, -33.23], [5.65, 0.18, 4.73, -33.24], [5.65, 0.19, 4.72, -33.26], [5.59, 0.18, 4.71, -33.28], [5.55, 0.17, 4.7, -33.3]]\n",
      "[[5.55, 0.17, 4.69, -33.31], [5.5, 0.16, 4.68, -33.32], [5.47, 0.16, 4.67, -33.33], [5.46, 0.16, 4.66, -33.33], [5.43, 0.15, 4.65, -33.34], [5.39, 0.15, 4.64, -33.34], [5.37, 0.15, 4.63, -33.35], [5.35, 0.15, 4.62, -33.37], [5.32, 0.14, 4.61, -33.38], [5.31, 0.14, 4.6, -33.39]]\n",
      "[[5.29, 0.14, 4.59, -33.41], [5.27, 0.14, 4.58, -33.43], [5.26, 0.14, 4.57, -33.44], [5.24, 0.14, 4.56, -33.44], [5.22, 0.14, 4.55, -33.45], [5.21, 0.13, 4.54, -33.46], [5.2, 0.13, 4.53, -33.47], [5.18, 0.13, 4.52, -33.48], [5.15, 0.13, 4.51, -33.49], [5.15, 0.13, 4.5, -33.51]]\n",
      "[[5.13, 0.13, 4.49, -33.52], [5.12, 0.13, 4.48, -33.53], [5.11, 0.13, 4.47, -33.53], [5.09, 0.13, 4.46, -33.54], [5.08, 0.13, 4.45, -33.55], [5.07, 0.12, 4.44, -33.56], [5.06, 0.13, 4.43, -33.57], [5.04, 0.12, 4.43, -33.58], [5.04, 0.12, 4.42, -33.59], [5.02, 0.12, 4.41, -33.59]]\n",
      "[[5.01, 0.12, 4.4, -33.6], [5.0, 0.12, 4.39, -33.61], [4.99, 0.12, 4.39, -33.62], [4.98, 0.12, 4.38, -33.63], [4.99, 0.12, 4.37, -33.64], [5.03, 0.13, 4.36, -33.64], [5.01, 0.13, 4.36, -33.64], [4.98, 0.13, 4.35, -33.65], [5.0, 0.13, 4.34, -33.66], [4.99, 0.13, 4.34, -33.66]]\n",
      "[[4.99, 0.13, 4.33, -33.67], [5.03, 0.14, 4.32, -33.68], [4.99, 0.14, 4.32, -33.7], [4.99, 0.14, 4.31, -33.7], [4.98, 0.14, 4.31, -33.7], [4.95, 0.13, 4.3, -33.71], [4.93, 0.13, 4.3, -33.71], [5.01, 0.14, 4.29, -33.73], [5.02, 0.15, 4.29, -33.74], [5.04, 0.15, 4.28, -33.76]]\n",
      "[[4.93, 0.13, 4.28, -33.76], [4.96, 0.14, 4.28, -33.76], [4.95, 0.13, 4.27, -33.77], [4.9, 0.13, 4.27, -33.77], [4.92, 0.13, 4.26, -33.78], [4.9, 0.13, 4.26, -33.78], [4.88, 0.12, 4.26, -33.78], [4.88, 0.13, 4.25, -33.78], [4.87, 0.12, 4.25, -33.78], [4.85, 0.12, 4.24, -33.8]]\n",
      "[[4.93, 0.14, 4.24, -33.81], [5.08, 0.17, 4.23, -33.82], [4.96, 0.15, 4.23, -33.82], [5.05, 0.16, 4.23, -33.82], [5.02, 0.16, 4.23, -33.82], [5.0, 0.15, 4.23, -33.82], [4.96, 0.15, 4.22, -33.83], [4.93, 0.14, 4.22, -33.83], [4.92, 0.14, 4.22, -33.83], [4.94, 0.14, 4.22, -33.83]]\n",
      "[[4.88, 0.13, 4.21, -33.83], [4.86, 0.13, 4.21, -33.83], [4.87, 0.13, 4.21, -33.83], [4.86, 0.13, 4.21, -33.84], [4.87, 0.13, 4.21, -33.84], [4.83, 0.12, 4.2, -33.83], [4.8, 0.12, 4.2, -33.82], [4.84, 0.13, 4.2, -33.81], [4.89, 0.14, 4.2, -33.81], [4.79, 0.12, 4.19, -33.81]]\n",
      "[[4.88, 0.14, 4.19, -33.82], [4.81, 0.12, 4.19, -33.81], [4.9, 0.14, 4.19, -33.81], [4.78, 0.12, 4.18, -33.8], [4.94, 0.15, 4.18, -33.81], [4.83, 0.13, 4.17, -33.8], [4.92, 0.15, 4.17, -33.81], [4.85, 0.14, 4.17, -33.84], [4.89, 0.15, 4.16, -33.86], [4.9, 0.15, 4.16, -33.88]]\n",
      "[[4.83, 0.13, 4.16, -33.88], [4.82, 0.13, 4.16, -33.88], [4.79, 0.13, 4.16, -33.87], [4.82, 0.13, 4.16, -33.87], [4.82, 0.13, 4.15, -33.88], [4.78, 0.13, 4.15, -33.88], [4.74, 0.12, 4.15, -33.89], [4.74, 0.12, 4.15, -33.9], [4.73, 0.12, 4.14, -33.9], [4.73, 0.12, 4.14, -33.9]]\n"
     ]
    }
   ],
   "source": [
    "noise = torch.zeros_like(audio)\n",
    "noise.requires_grad = True\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "reg_weight = 1\n",
    "ctc_weight = 5\n",
    "eps = 10\n",
    "\n",
    "losses = []\n",
    "min_loss = 100000\n",
    "min_noise = None\n",
    "for i in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    loss, ctc_loss_value, noise_reg, dB_x_delta = loss_function(audio, noise, target_logits, model, reg_weight, ctc_weight, eps)\n",
    "    if loss is None:\n",
    "        break\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    itemized_loss = loss.item()\n",
    "    losses_to_observe = [round(itemized_loss,2), round(ctc_loss_value,2), round(noise_reg,2), round(dB_x_delta,2)]\n",
    "    losses.append(losses_to_observe)\n",
    "    if itemized_loss < min_loss:\n",
    "        min_loss = itemized_loss\n",
    "        min_noise = noise.detach().cpu().numpy()\n",
    "    if i % 10 == 0:\n",
    "        print(losses[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the last noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR']\n"
     ]
    }
   ],
   "source": [
    "# test = audio + torch.tensor(min_noise).to(device)\n",
    "test = audio + noise\n",
    "logits = model(test).logits\n",
    "#print predicted sentence\n",
    "print(processor.batch_decode(torch.argmax(logits, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"test.wav\", test.detach().cpu(), sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9, 'I': 10, 'H': 11, 'S': 12, 'R': 13, 'D': 14, 'L': 15, 'U': 16, 'M': 17, 'W': 18, 'C': 19, 'F': 20, 'G': 21, 'Y': 22, 'P': 23, 'B': 24, 'V': 25, 'K': 26, \"'\": 27, 'X': 28, 'J': 29, 'Q': 30, 'Z': 31}\n"
     ]
    }
   ],
   "source": [
    "#get processor dictionary\n",
    "processor_dict = processor.tokenizer.get_vocab()\n",
    "print(processor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERYTHING BELOW IS TRASH DO NOT BOTHER WITH IT\n",
    "\n",
    "unless you need inspiration or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 15.0521, -29.1595, -28.8068,  ...,  -7.1279,  -7.7091,  -7.7615],\n",
       "         [ 15.0937, -29.1127, -28.7568,  ...,  -6.8847,  -7.6604,  -7.6667],\n",
       "         [ 15.2429, -28.5604, -28.2161,  ...,  -6.1149,  -7.0726,  -7.4412],\n",
       "         ...,\n",
       "         [ 15.2458, -28.5528, -28.2087,  ...,  -6.1091,  -7.0709,  -7.4314],\n",
       "         [ 15.0701, -29.5002, -29.1428,  ...,  -7.5385,  -8.4064,  -7.8453],\n",
       "         [ 15.0077, -29.4751, -29.1234,  ...,  -7.4993,  -8.3574,  -7.8958]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"][\"sampling_rate\"]).input_values.to(device)).logits\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"MIXSTER QUVILTZYRY VIZS THE APOSTLE B<s>F THY MIDDLEY CLASSZES' BAND WEYV'RE GVAD TO WELCOMEYB HISZS JOXBEBL'Y</s>Y\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = softmax(res)\n",
    "res = torch.argmax(res, dim=2)\n",
    "sentence = processor.batch_decode(res)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17, 17,  0, 10, 28, 12,  0,  6,\n",
       "          0,  5, 13, 13,  4,  4,  0, 30, 16, 16,  0, 25, 10, 15, 15,  0,  0,  6,\n",
       "         31, 31, 31, 22, 13, 13, 22, 22,  4,  4,  0, 25, 10, 31, 12,  4,  4,  4,\n",
       "          6, 11,  5,  5,  4,  4,  0,  0,  7,  0,  0,  0, 23,  0,  0,  0,  0,  0,\n",
       "          8, 12, 12,  0,  0,  6,  6,  0, 15, 15, 15,  5,  5,  4,  4,  4,  4, 24,\n",
       "         24,  1, 20,  0,  4,  4,  4,  6, 11, 22,  4,  4, 17, 17, 10, 14,  0,  0,\n",
       "         14, 15, 15,  5, 22, 22,  4, 19,  0, 15, 15,  0,  0,  0,  0,  0,  7, 12,\n",
       "         12,  0,  0, 12, 31,  0,  0,  0,  5,  0, 12, 12, 27,  4,  4,  0,  0,  0,\n",
       "          0,  0,  0, 24, 24, 24,  7,  9, 14, 14,  4,  4, 18, 18,  0,  5, 22, 25,\n",
       "         27, 27, 13,  5,  5,  4,  4, 21,  0, 25,  0,  0,  0,  0,  0,  7,  0,  0,\n",
       "         14, 14, 14,  4,  4,  4,  0,  6,  0,  8,  8,  4,  4,  0, 18,  0,  0,  5,\n",
       "          0, 15, 15,  0,  0, 19, 19,  0,  8, 17, 17,  5,  5, 22, 24,  4,  4, 11,\n",
       "         11,  0,  0, 10, 12, 31, 12,  4,  4,  4,  0, 29,  0,  0,  0,  0,  0,  8,\n",
       "          8, 28, 28,  0,  0,  0, 24,  0,  0,  0,  5,  5, 24, 15, 15, 27, 22,  2,\n",
       "         22,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 11,\n",
       " 5,\n",
       " 3,\n",
       " 19,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 12,\n",
       " 3,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 10,\n",
       " 14,\n",
       " 5,\n",
       " 3,\n",
       " 17,\n",
       " 22,\n",
       " 3,\n",
       " 24,\n",
       " 7,\n",
       " 21,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 14,\n",
       " 3,\n",
       " 10,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 8,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 11,\n",
       " 5,\n",
       " 3,\n",
       " 20,\n",
       " 15,\n",
       " 8,\n",
       " 8,\n",
       " 13]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.convert_tokens_to_ids([c for c in \"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE<unk>A<unk>IS<unk>INSIDE<unk>MY<unk>BAG</s>ANDZIT<s>ROLLS<unk>ON<unk>THE<unk>FLOOR']\n"
     ]
    }
   ],
   "source": [
    "test = audio + torch.tensor(min_noise).to(device)\n",
    "# test = audio + noise\n",
    "logits = model(test).logits\n",
    "#print predicted sentence\n",
    "print(processor.batch_decode(torch.argmax(logits, dim=-1)))\n",
    "torchaudio.save(\"best_test.wav\", test.detach().cpu(), sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = ds[0][\"text\"]\n",
    "target_sentence = [c for c in target_sentence]\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target_sentence)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "test_target_sentence = ds[5]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "softmaxed_logits = softmax(output_logits)\n",
    "\n",
    "# show predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(softmaxed_logits, dim=-1))\n",
    "print(predicted_sentence)\n",
    "print(ds[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.7382, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss(output_logits.unsqueeze(1), target_logits.unsqueeze(0), torch.tensor([output_logits.shape[0]]), torch.tensor([target_logits.shape[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[89]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[target_logits.shape[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n"
     ]
    }
   ],
   "source": [
    "output_logits = model(audio).logits\n",
    "output_logits = softmax(output_logits[0])\n",
    "\n",
    "# get predicted sentence\n",
    "predicted_sentence = processor.decode(torch.argmax(output_logits, dim=-1))\n",
    "print(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1263.2513, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_target_sentence = ds[1]['text']\n",
    "test_target_sentence = ds[2]['text']\n",
    "test_target_sentence = [c for c in test_target_sentence]\n",
    "test_target_logits = processor.tokenizer.convert_tokens_to_ids(test_target_sentence)\n",
    "test_target_logits = torch.tensor(test_target_logits)\n",
    "\n",
    "ctc_loss(output_logits, test_target_logits.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate = ds[0][\"audio\"]['sampling_rate']).input_values  # Batch size 1\n",
    "\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torch.Tensor(ds[0][\"audio\"][\"array\"]).unsqueeze(0)\n",
    "rate = ds[0][\"audio\"][\"sampling_rate\"]\n",
    "torchaudio.save(\"test.wav\", audio, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_transcription = 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n",
    "\n",
    "transcription = \"THE CAT JUMPED OVER THE FOX WHERE IT HAS SHOWN US THE WORLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = processor.tokenizer.convert_tokens_to_ids([c for c in transcription[0]])\n",
    "target = torch.Tensor(target).unsqueeze(0).long()\n",
    "target_shape = target[0].shape[0]\n",
    "logits_shape = logits[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-170.5979, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctcloss(log_probs=softmax(logits[0]), targets=target[0], input_lengths=[logits_shape], target_lengths=[target_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_characters = processor.tokenizer.convert_ids_to_tokens(predicted_ids[0].tolist())\n",
    "predicted_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_list = [c for c in transcription[0]]\n",
    "transcription_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.convert_tokens_to_ids(transcription_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# remove consecutive duplicates\n",
    "result = [k for k, g in itertools.groupby(predicted_ids[0])]\n",
    "# remove blanks\n",
    "result = [x for x in result if x != 0]\n",
    "#count\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_ids>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert transcription to logits\n",
    "transcription_logits = processor(transcription, return_tensors=\"pt\", padding=\"longest\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files extra2a, extra2b and infer\n",
    "files = [\"extra2a.wav\", \"extra2b.wav\"]\n",
    "# read audios\n",
    "audio, rate = torchaudio.load(files[0])\n",
    "audio2, rate2 = torchaudio.load(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer audio1\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 93680])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THAT DAY THE MERCHANT GAVE THE BOY PERMISSION TO BUILD THE DIS']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#infer audio1\n",
    "file = \"extra_0a.wav\"\n",
    "audio, rate = torchaudio.load(file)\n",
    "audio = audio\n",
    "input_values = processor(audio[0], return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values  # Batch size 1\n",
    "# retrieve logits\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Minimize \n",
    "\n",
    "$ dB_x(\\delta) + c l(x+\\delta, t) $\n",
    "\n",
    "where \n",
    "\n",
    "$dB_x(\\delta)$ is the strength of the noise compared to the signal\n",
    "\n",
    "$c$: tradeoff parameter between being adversarial and being close to the original signal\n",
    "\n",
    "$l(x+\\delta, t)$ : the loss between the (disturbed signal prediction?) and the target sentence to become adversarial t?\n",
    "\n",
    "we define $l$ as the CTC-loss, so we can say:\n",
    "\n",
    "$-log Pr(t | x+\\delta) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"THE CAT IS INSIDE MY BAG AND IT ROLLS ON THE FLOOR\"]\n",
    "#assuming: target is a list which contains one sentence\n",
    "target = [c for c in target[0]]\n",
    "# convert to tensor logits\n",
    "target_logits = processor.tokenizer.convert_tokens_to_ids(target)\n",
    "target_logits = torch.tensor(target_logits)\n",
    "\n",
    "#compute adversarial example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "audio = audio.to(device)\n",
    "audio = processor(audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=rate).input_values\n",
    "audio = audio[0].to(device)\n",
    "target_logits = target_logits.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize dB_x(delta) - logPr(t|f(x+delta)), such that dB_x(delta) < eps\n",
    "# delta = argmin dB_x(delta) - logPr(t|f(x+delta))\n",
    "# dB_x(delta) = 20*log10(||x+delta||_2 / ||delta||_2)\n",
    "# x = audio\n",
    "# t = target transcription\n",
    "# f = model\n",
    "# eps = max distortion\n",
    "# delta = perturbation\n",
    "\n",
    "\n",
    "# define loss function\n",
    "def loss_function(audio, noise, target_logits, model, eps, ctc_constant):\n",
    "    audio_perturbed = audio + noise\n",
    "    # print(input_values.shape)\n",
    "    # print(audio_perturbed.shape)\n",
    "    #audio: clean audio\n",
    "    # calculate dB_x, dB_delta, dB_x(delta) , where delta is perturbed_noise - clean_audio\n",
    "    dB_x = 20*torch.log10(torch.norm(audio))\n",
    "    # calculate dB_delta\n",
    "    # add 1e-10 to avoid log of zero\n",
    "    dB_delta = 20*torch.log10(torch.norm(noise+1e-10))\n",
    "    # calculate dB_x(delta)\n",
    "    dB_x_delta = dB_delta - dB_x\n",
    "\n",
    "    # calculate logPr(t|f(x+delta))\n",
    "    logits = model(audio_perturbed).logits\n",
    "    logits = logits[0] # remove batch dimension\n",
    "    logits = softmax(logits)\n",
    "    # print(target_logits)\n",
    "    # print(target_logits.shape, target_logits.dtype)\n",
    "    # print(logits)\n",
    "    # print(logits.shape, logits.dtype)\n",
    "\n",
    "    # print(logits.shape[0])\n",
    "    # print(target_logits.shape[0])\n",
    "    # print(target_logits)\n",
    "    # print(logits)\n",
    "    logPr = ctcloss(logits, target_logits, [logits.shape[0]], [target_logits.shape[0]])\n",
    "    # calculate loss\n",
    "    # print(\"dB_x_delta, logPr\")\n",
    "    # print(dB_x_delta, logPr)\n",
    "    # loss = dB_x_delta - ctc_constant * logPr\n",
    "    loss = - logPr\n",
    "\n",
    "    # check if dbloss is smaller than eps\n",
    "    if dB_x_delta < eps:\n",
    "        return loss\n",
    "    else:\n",
    "        # print(dB_x_delta, eps)\n",
    "        # return None\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss\n",
      "tensor(5.8390, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "final loss\n",
      "tensor(5.9684, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctc_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctc_constant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# break if loss is None\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[25], line 38\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(audio, noise, target_logits, model, eps, ctc_constant)\u001b[0m\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(target_logits.shape, target_logits.dtype)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# print(target_logits)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(logits)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m logPr \u001b[38;5;241m=\u001b[39m \u001b[43mctcloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"dB_x_delta, logPr\")\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# print(dB_x_delta, logPr)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# loss = dB_x_delta - ctc_constant * logPr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m logPr\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1770\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deus-diabolus\\miniconda3\\envs\\uni_env\\Lib\\site-packages\\torch\\nn\\functional.py:2656\u001b[0m, in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[0;32m   2650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2651\u001b[0m         ctc_loss,\n\u001b[0;32m   2652\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   2653\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[0;32m   2654\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[0;32m   2655\u001b[0m     )\n\u001b[1;32m-> 2656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define eps\n",
    "eps = 10\n",
    "# define number of iterations\n",
    "n = 5000\n",
    "# define learning rate\n",
    "lr = 1e-1\n",
    "# define perturbed audio: start with clean audio\n",
    "noise = torch.zeros_like(audio).requires_grad_(True)\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([noise], lr=lr)\n",
    "ctc_constant = 1\n",
    "\n",
    "# loop over n iterations\n",
    "for i in range(n):\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # calculate loss\n",
    "    loss = loss_function(audio, noise, target_logits, model, eps, ctc_constant=ctc_constant)\n",
    "    # break if loss is None\n",
    "    if loss is None:\n",
    "        break\n",
    "    print(\"final loss\")\n",
    "    print(loss)      \n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update perturbation\n",
    "    optimizer.step()\n",
    "#    print(audio_pert)\n",
    "# save adversarial example\n",
    "audio_pert = (audio+noise).detach().to(\"cpu\")\n",
    "torchaudio.save(\"adversarial_one.wav\", audio_pert, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.0689, device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3960265 ,  0.37243792, -0.37965736, ...,  0.33345932,\n",
       "        -0.42485094,  0.32958943]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3785, -0.3786, -0.3785,  ..., -0.4242, -0.4231, -0.4313]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display audio object\n",
    "audio_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.0002,  0.0002,  ..., -0.0454, -0.0443, -0.0526]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxed = torch.nn.Softmax(dim=1)\n",
    "probs = softmaxed(model(perturbed_audio).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"THE WILL BURN YOU TO A CRISP\"\n",
    "#convert to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_audio = audio + audio_pert\n",
    "dB_x = 20 * torch.log10(torch.norm(audio) / torch.norm(audio_pert))\n",
    "# calculate logPr(t|f(x+delta))\n",
    "logits = model(perturbed_audio).logits\n",
    "pred = processor.batch_decode(torch.argmax(logits, dim=-1))\n",
    "\n",
    "#print(target)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
